---
layout: article
title: "Paper reading: BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning"
date: 2021-06-09
tags: paper review SNN 
mathjax: true
key: 2021-06-09-blog
weight: 0
---
## 核心思想
SNN的神经元（即激活函数）输出的是脉冲，也就是二进制值，这使得SNN在训练和推理的计算量上相比于ANN降低了很多。同时，如果将权重进行量化，可以进一步降低计算量，其中一种量化后的SNN为BiSNN(Binary SNN)，BiSNN的权重为1 bit(+1 or -1)。    
但是，SNN也存在着由于激活函数不可微分而难以训练的问题。大部分工作的做法是使用近似梯度通过时序反向传播训练。本文针对BiSNN(binary SNN)提出了Bayes-BiSNN训练算法，可以利用梯度下降直接训练BiSNN。同时，本文将直通滤波器和近似梯度结合起来，提出了一种对BiSNN的的训练算法ST-BiSNN。最后，本文对两种训练算法进行了对比。
## 实现过程
两种训练算法均针对监督学习设计，即输入为数据集的数据标签对和超参数，输出学习到的1 bit权重。BiSNN的权重初始化为实数值，在训练过程中，重复以下步骤直到算法收敛。具体的，从数据集中选出一个序列输入，首先对权重二值化，然后利用监督学习的反向传播的训练范式进行梯度下降，其中，梯度计算利用的是二值化的权重，并用计算的梯度更新实数值权重。ST-BiSNN算法和Bayes-BiSNN的权重二值化过程、梯度计算过程和参数更新算法不同，但整体流程一样。
## 测试任务
两个训练算法分别在一维和二维数据上进行了测试，并最后在MNIST-DSV数据集上进行了测试。
## 创新点
本文的创新点在于两个梯度更新算法的理论分析，尤其是Bayes-BiSNN，从理论上证明了可以通过反向传播直接训练BiSNN。
## 算法评价
ST-BiSNN和Bayes-BiSNN是两个直接对BiSNN进行训练的算法，相比于ANN-SNN转换方法，更加高效食用，同时也为SNN训练算法设计提供了一些思路。