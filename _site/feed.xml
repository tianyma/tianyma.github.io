<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2021-06-07T09:14:16+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">tianyma’s blog</title><subtitle>do valuable research, be valuable person.
</subtitle><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><entry><title type="html">Paper reading: Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning</title><link href="http://localhost:4000/2021/06/06/savn.html" rel="alternate" type="text/html" title="Paper reading: Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning" /><published>2021-06-06T00:00:00+08:00</published><updated>2021-06-06T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/06/savn</id><content type="html" xml:base="http://localhost:4000/2021/06/06/savn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;本文是meta-RL在navigation中的应用。  &lt;br /&gt;
本文将navigation任务的学习分为了三个层次：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learn：通过监督式actor-critic loss进行学习&lt;/li&gt;
  &lt;li&gt;learn to learn：通过自适应算法的interactive loss进行学习&lt;/li&gt;
  &lt;li&gt;learn to how to learn:通过学习自适应算法的interactive loss进行学习&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文提出的SAVN(self-adapted visual navigation)在传统的强化学习算法（如A2C）的基础上，添加了基于梯度下降的元学习算法（based on MAML）和自监督学习机制，让agent可以在通过与环境交互来学习的基础上，达成其他两个更高层次的能力：学会学习，学会如何学习。&lt;/p&gt;

&lt;p&gt;通俗的说，让agent学会了在新环境中随机应变的能力，使其可以快速适应新环境。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;在训练过程中，通过最小化interactive loss来学习自适应网络，在测试过程中，固定自适应网络参数，只学习优化navigation loss。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文在AI-thor的3D navigation任务上进行了测试，任务是只基于RGB观测信息导航agent到任一指定物品处。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;在navigation中加入meta-RL的概念，通过自监督学习使agent在inference阶段也可以学习更新策略。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;本文在传统RL navigation的基础上加入了meta learning和自监督学习的概念，是meta-RL在navigation的一个不错的应用。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 本文是meta-RL在navigation中的应用。 本文将navigation任务的学习分为了三个层次： learn：通过监督式actor-critic loss进行学习 learn to learn：通过自适应算法的interactive loss进行学习 learn to how to learn:通过学习自适应算法的interactive loss进行学习 本文提出的SAVN(self-adapted visual navigation)在传统的强化学习算法（如A2C）的基础上，添加了基于梯度下降的元学习算法（based on MAML）和自监督学习机制，让agent可以在通过与环境交互来学习的基础上，达成其他两个更高层次的能力：学会学习，学会如何学习。 通俗的说，让agent学会了在新环境中随机应变的能力，使其可以快速适应新环境。 实现过程 在训练过程中，通过最小化interactive loss来学习自适应网络，在测试过程中，固定自适应网络参数，只学习优化navigation loss。 测试任务 本文在AI-thor的3D navigation任务上进行了测试，任务是只基于RGB观测信息导航agent到任一指定物品处。 创新点 在navigation中加入meta-RL的概念，通过自监督学习使agent在inference阶段也可以学习更新策略。 算法评价 本文在传统RL navigation的基础上加入了meta learning和自监督学习的概念，是meta-RL在navigation的一个不错的应用。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</title><link href="http://localhost:4000/2021/06/05/dream.html" rel="alternate" type="text/html" title="Paper reading: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices" /><published>2021-06-05T00:00:00+08:00</published><updated>2021-06-05T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/05/dream</id><content type="html" xml:base="http://localhost:4000/2021/06/05/dream.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;想象要学习一门新的编程语言，一种做法是找一本教科书或找到官方文档，完整阅读一遍，然后开始编程，但这也许不是最高效的做法。更高效的做法是找到一个要解决的问题或要实现的算法，尝试写出伪代码，然后学习这门语言的语法，并用其实现这个算法，也就是在实践中去学习。这个过程可以抽象成一个exploration和exploitation的过程。  &lt;br /&gt;
在meta-RL中，存在如何解耦exploration和exploitation的问题，如教会一个机器人做饭，我们可以让其在一些厨房场景中先学习如何做饭，然后将其放在新的场景下，让其快速适应并完成任务。在新的场景中，机器人要找到食材和厨具（exploration），然后做出一道可口的菜（exploitation）。  &lt;br /&gt;
虽然通常的meta-RL的算法对exploration和exploitation进行了解耦，但在explore的时候往往会收集到任务无关的信息而陷入局部最优解。  &lt;br /&gt;
DREAM的核心思想是构建独立的exploitation目标来确定任务相关的信息，然后构建独立的explore目标来寻找这些信息，从而实现最优解。通俗的说，就是带着目的去探索，然后用探索到的信息去完成任务。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;在meta-training的trial中，针对于一个场景问题$\mu$，Dream训练一个任务编码器$F_{\psi}$，该编码器可以提取出$\mu$中的任务相关的信息，利用$F_{\psi}$作为中间件将exploration和exploitation进行解耦。  &lt;br /&gt;
定义相关信息函数$I$，在exploration的过程中，通过更新exploration的策略来最大化该相关信息函数，从而得到最优exploration策略。 &lt;br /&gt;
在exploitation的过程中，通过更新exploitation的策略和$F_{\psi}$来最大化exploitation奖励，从而得到最优exploitation策略。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;Dream分别在定制的2D navigation环境和基于gym-miniworld的3D navigation环境上进行了测试。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文提出了一个任务编码器，可以从问题中提取出任务相关信息，并排除任务无关信息，从而通过解耦exploration和exploitation，分别对两个过程进行优化得到最优策略。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;Dream针对于端到端的meta-train方法容易陷入局部最优并且需要大量训练样本的问题，对exploration和exploitation进行了解耦，从而可以分别对这两个过程的策略进行优化。本文对问题定义很清晰，算法也很符合直觉。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 想象要学习一门新的编程语言，一种做法是找一本教科书或找到官方文档，完整阅读一遍，然后开始编程，但这也许不是最高效的做法。更高效的做法是找到一个要解决的问题或要实现的算法，尝试写出伪代码，然后学习这门语言的语法，并用其实现这个算法，也就是在实践中去学习。这个过程可以抽象成一个exploration和exploitation的过程。 在meta-RL中，存在如何解耦exploration和exploitation的问题，如教会一个机器人做饭，我们可以让其在一些厨房场景中先学习如何做饭，然后将其放在新的场景下，让其快速适应并完成任务。在新的场景中，机器人要找到食材和厨具（exploration），然后做出一道可口的菜（exploitation）。 虽然通常的meta-RL的算法对exploration和exploitation进行了解耦，但在explore的时候往往会收集到任务无关的信息而陷入局部最优解。 DREAM的核心思想是构建独立的exploitation目标来确定任务相关的信息，然后构建独立的explore目标来寻找这些信息，从而实现最优解。通俗的说，就是带着目的去探索，然后用探索到的信息去完成任务。 实现过程 在meta-training的trial中，针对于一个场景问题$\mu$，Dream训练一个任务编码器$F_{\psi}$，该编码器可以提取出$\mu$中的任务相关的信息，利用$F_{\psi}$作为中间件将exploration和exploitation进行解耦。 定义相关信息函数$I$，在exploration的过程中，通过更新exploration的策略来最大化该相关信息函数，从而得到最优exploration策略。 在exploitation的过程中，通过更新exploitation的策略和$F_{\psi}$来最大化exploitation奖励，从而得到最优exploitation策略。 测试任务 Dream分别在定制的2D navigation环境和基于gym-miniworld的3D navigation环境上进行了测试。 创新点 本文提出了一个任务编码器，可以从问题中提取出任务相关信息，并排除任务无关信息，从而通过解耦exploration和exploitation，分别对两个过程进行优化得到最优策略。 算法评价 Dream针对于端到端的meta-train方法容易陷入局部最优并且需要大量训练样本的问题，对exploration和exploitation进行了解耦，从而可以分别对这两个过程的策略进行优化。本文对问题定义很清晰，算法也很符合直觉。 返回文章列表</summary></entry><entry><title type="html">Paper reading: On First-Order Meta-Learning Algorithms</title><link href="http://localhost:4000/2021/06/04/Reptile.html" rel="alternate" type="text/html" title="Paper reading: On First-Order Meta-Learning Algorithms" /><published>2021-06-04T00:00:00+08:00</published><updated>2021-06-04T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/04/Reptile</id><content type="html" xml:base="http://localhost:4000/2021/06/04/Reptile.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;Reptile和MAML一样都是模型无关的元学习算法，不同的是，Reptile是用在新任务上的梯度下降方向和初始参数的方向求差作为初始参数的最终更新方向。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/post_assets/2021-06-04/gradient-update-reptile.jpg&quot; alt=&quot;pre-erain、MAML、Reptile的梯度更新方向&quot; /&gt;
  &lt;figcaption&gt;Pre-erain、MAML、Reptile的梯度更新方向的示意图。$\theta$为初始参数，蓝色箭头为不同方法初始参数更新方向，绿色箭头为初始参数在采样的任务上的梯度下降方向。&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;算法流程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;采样一个任务或一批任务。&lt;/li&gt;
  &lt;li&gt;在任务上用初始化参数基于梯度下降训练，得到所有梯度方向，并求出总的梯度下降方向。&lt;/li&gt;
  &lt;li&gt;基于初始参数方向和总梯度下降方向的差作为参数更新方向。
    &lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;作者针对于few-shot classification做了实验，效果和MAML差不多。
    &lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
    &lt;p&gt;Reptile是基于MAML做了稍微改进，用梯度下降方向和初始参数方向的差作为更新方向，优势在于兼顾了所有任务的更新，降低了overfit的风险。&lt;/p&gt;
    &lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
    &lt;p&gt;Reptile基于MAML做了微小改动，核心思想并没有太大差别。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 Reptile和MAML一样都是模型无关的元学习算法，不同的是，Reptile是用在新任务上的梯度下降方向和初始参数的方向求差作为初始参数的最终更新方向。 实现过程 Pre-erain、MAML、Reptile的梯度更新方向的示意图。$\theta$为初始参数，蓝色箭头为不同方法初始参数更新方向，绿色箭头为初始参数在采样的任务上的梯度下降方向。 算法流程： 采样一个任务或一批任务。 在任务上用初始化参数基于梯度下降训练，得到所有梯度方向，并求出总的梯度下降方向。 基于初始参数方向和总梯度下降方向的差作为参数更新方向。 测试任务 作者针对于few-shot classification做了实验，效果和MAML差不多。 创新点 Reptile是基于MAML做了稍微改进，用梯度下降方向和初始参数方向的差作为更新方向，优势在于兼顾了所有任务的更新，降低了overfit的风险。 算法评价 Reptile基于MAML做了微小改动，核心思想并没有太大差别。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title><link href="http://localhost:4000/2021/06/03/MAML.html" rel="alternate" type="text/html" title="Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" /><published>2021-06-03T00:00:00+08:00</published><updated>2021-06-03T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/03/MAML</id><content type="html" xml:base="http://localhost:4000/2021/06/03/MAML.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 &lt;br /&gt;
MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。  &lt;br /&gt;
由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。 实现过程 以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。 由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。 测试任务 在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。 创新点 MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。 算法评价 MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。 返回文章列表</summary></entry><entry><title type="html">Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning</title><link href="http://localhost:4000/2021/06/02/RL2.html" rel="alternate" type="text/html" title="Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning" /><published>2021-06-02T00:00:00+08:00</published><updated>2021-06-02T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/02/RL2</id><content type="html" xml:base="http://localhost:4000/2021/06/02/RL2.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;episode:&lt;/strong&gt; agent和一个MDP的一次开始到结束的交互序列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;trial:&lt;/strong&gt; agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。&lt;/p&gt;

&lt;p&gt;agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了&lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;TRPO算法&lt;/a&gt;。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-armed Bandit&lt;/li&gt;
  &lt;li&gt;Tabular MDP&lt;/li&gt;
  &lt;li&gt;3D maze navigation(ViZDoom environment)
    &lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
    &lt;p&gt;这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。&lt;/p&gt;
    &lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
    &lt;p&gt;这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。 实现过程 定义 episode: agent和一个MDP的一次开始到结束的交互序列。 trial: agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。 agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了TRPO算法。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。 测试任务 Multi-armed Bandit Tabular MDP 3D maze navigation(ViZDoom environment) 创新点 这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。 算法评价 这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Learning to reinforcement learn</title><link href="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html" rel="alternate" type="text/html" title="Paper reading: Learning to reinforcement learn" /><published>2021-06-01T00:00:00+08:00</published><updated>2021-06-01T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn</id><content type="html" xml:base="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。&lt;/p&gt;

&lt;p&gt;训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的&lt;a href=&quot;https://scienceofbehaviorchange.org/measures/two-stage-task/&quot;&gt;two step task&lt;/a&gt;和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。 实现过程 本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。 训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。 测试任务 本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的two step task和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。 创新点 本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。 算法评价 返回文章列表</summary></entry><entry><title type="html">Summary: Reinforcement learning benchmarks</title><link href="http://localhost:4000/2021/05/30/RL-benchmark.html" rel="alternate" type="text/html" title="Summary: Reinforcement learning benchmarks" /><published>2021-05-30T00:00:00+08:00</published><updated>2021-05-30T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/30/RL-benchmark</id><content type="html" xml:base="http://localhost:4000/2021/05/30/RL-benchmark.html">&lt;p&gt;这是一篇强化学习benchmark的总结，按照任务对其进行分类，并不全面地列举了在benchmark测试过的文章及开源代码。&lt;/p&gt;
&lt;h2 id=&quot;2d-navigation&quot;&gt;2D navigation&lt;/h2&gt;
&lt;h3 id=&quot;gym-minigrid&quot;&gt;gym-minigrid&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;source code: &lt;a href=&quot;https://github.com/maximecb/gym-minigrid&quot;&gt;https://github.com/maximecb/gym-minigrid&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;scene screenshot:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/post_assets/2021-05-30/minigrid-shot.gif&quot; alt=&quot;gym-minigrid-shot&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use cases:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.07882&quot;&gt;Guiding Policies with Language via Meta-Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gym-maze&quot;&gt;gym-maze&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;source code: &lt;a href=&quot;https://github.com/MattChanTK/gym-maze&quot;&gt;https://github.com/MattChanTK/gym-maze&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;scene screenshot:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/post_assets/2021-05-30/gymmaze-shot.gif&quot; alt=&quot;gym-maze-shot&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use cases:
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!---   
### 
- source code: 
- scene screenshot: 
- use cases:
  - 
-----&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/clvrai/awesome-rl-envs#navigation&quot;&gt;https://github.com/clvrai/awesome-rl-envs#navigation&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="RL-benchmark" /><summary type="html">这是一篇强化学习benchmark的总结，按照任务对其进行分类，并不全面地列举了在benchmark测试过的文章及开源代码。 2D navigation gym-minigrid source code: https://github.com/maximecb/gym-minigrid scene screenshot: use cases: Guiding Policies with Language via Meta-Learning gym-maze source code: https://github.com/MattChanTK/gym-maze scene screenshot: use cases: Reference: https://github.com/clvrai/awesome-rl-envs#navigation</summary></entry><entry><title type="html">Paper review: meta-reinforcement-learning</title><link href="http://localhost:4000/2021/05/29/meta-reinforcement-learning.html" rel="alternate" type="text/html" title="Paper review: meta-reinforcement-learning" /><published>2021-05-29T00:00:00+08:00</published><updated>2021-05-29T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/29/meta-reinforcement-learning</id><content type="html" xml:base="http://localhost:4000/2021/05/29/meta-reinforcement-learning.html">&lt;h2 id=&quot;challenges-of-meta-rl&quot;&gt;Challenges of meta-RL&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;design a set of tasks that are interrelated&lt;/li&gt;
  &lt;li&gt;find the inter-representation&lt;/li&gt;
  &lt;li&gt;fast adaptation to new tasks
    &lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;
    &lt;h3 id=&quot;environment&quot;&gt;environment&lt;/h3&gt;
    &lt;h4 id=&quot;meta-world-a-benchmark-and-evaluation-for-multi-task-and-meta-reinforcement-learning&quot;&gt;Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;source: PMLR 2020&lt;/li&gt;
  &lt;li&gt;method: None&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;object manipulation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;http://proceedings.mlr.press/v100/yu20a/yu20a.pdf&quot;&gt;http://proceedings.mlr.press/v100/yu20a/yu20a.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:  &lt;a href=&quot;https://github.com/rlworkgroup/metaworld&quot;&gt;https://github.com/rlworkgroup/metaworld&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://meta-world.github.io/&quot;&gt;https://meta-world.github.io/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-based-meta-rl&quot;&gt;model-based meta-RL&lt;/h3&gt;
&lt;h4 id=&quot;learning-to-reinforcement-learn&quot;&gt;&lt;a href=&quot;/2021/06/01/Learning-to-reinforcement-learn.html&quot;&gt;Learning to reinforcement learn&lt;/a&gt;&lt;/h4&gt;
&lt;!-- #### [Learning to reinforcement learn](http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html) --&gt;
&lt;ul&gt;
  &lt;li&gt;source: CogSci 2017&lt;/li&gt;
  &lt;li&gt;method: deep meta-RL&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;bandit problem&lt;/li&gt;
      &lt;li&gt;Two-step task&lt;/li&gt;
      &lt;li&gt;Harlow experiment&lt;/li&gt;
      &lt;li&gt;3D navigation (Deepmind Lab)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1611.05763.pdf&quot;&gt;https://arxiv.org/pdf/1611.05763.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/Phu-archive/Learning2RL&quot;&gt;https://github.com/Phu-archive/Learning2RL&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;rl2-fast-reinforcement-learning-via-slow-reinforcement-learning&quot;&gt;&lt;a href=&quot;http://localhost:4000/2021/06/02/RL2.html&quot;&gt;RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2017&lt;/li&gt;
  &lt;li&gt;method: $RL^2$&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;multi-armed bandit problem&lt;/li&gt;
      &lt;li&gt;tabular MDP&lt;/li&gt;
      &lt;li&gt;3D navigation (ViZDoom)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1611.02779.pdf&quot;&gt;https://arxiv.org/pdf/1611.02779.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/mwufi/meta-rl-bandits&quot;&gt;https://github.com/mwufi/meta-rl-bandits (pytorch)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/VashishtMadhavan/rl2&quot;&gt;https://github.com/VashishtMadhavan/rl2 (tensorflow)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32606591&quot;&gt;https://zhuanlan.zhihu.com/p/32606591&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=HkLXCE9lx&quot;&gt;https://openreview.net/forum?id=HkLXCE9lx&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;prefrontal-cortex-as-a-meta-reinforcement-learning-system&quot;&gt;Prefrontal cortex as a meta-reinforcement learning system&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: Nature Neuroscience 2018&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://www.nature.com/articles/s41593-018-0147-8&quot;&gt;https://www.nature.com/articles/s41593-018-0147-8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;a-simple-neural-attentive-meta-learner&quot;&gt;A Simple Neural Attentive Meta-Learner&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2018&lt;/li&gt;
  &lt;li&gt;method: SNAIL (simple neural attentive learner)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;navigation&lt;/li&gt;
      &lt;li&gt;robotic locomotion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=B1DmUzWAW&quot;&gt;https://openreview.net/pdf?id=B1DmUzWAW&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/eambutu/snail-pytorch&quot;&gt;https://github.com/eambutu/snail-pytorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.carsi.edu.cn/index_zh.htm&quot;&gt;https://www.carsi.edu.cn/index_zh.htm&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pixelsnail-an-improved-autoregressive-generative-model&quot;&gt;PixelSNAIL: An Improved Autoregressive Generative Model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML 2018&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1712.09763v1.pdf&quot;&gt;https://arxiv.org/pdf/1712.09763v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/neocxi/pixelsnail-public&quot;&gt;https://github.com/neocxi/pixelsnail-public&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;concurrent-meta-reinforcement-learning&quot;&gt;Concurrent Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: arXiv:1903.02710 preprint&lt;/li&gt;
  &lt;li&gt;method: CMRL&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;N-Monty-Hall&lt;/li&gt;
      &lt;li&gt;N-Color-Choice&lt;/li&gt;
      &lt;li&gt;N-Reacher (Reacher-V2 from gym)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1903.02710v1.pdf&quot;&gt;https://arxiv.org/pdf/1903.02710v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reinforcement-learning-fast-and-slow&quot;&gt;Reinforcement Learning, Fast and Slow&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: Trends in Cognitive Sciences 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0&quot;&gt;https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;improving-generalization-in-meta-reinforcement-learning-using-learned-objectives&quot;&gt;Improving Generalization in Meta Reinforcement Learning using Learned Objectives&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1910.04098.pdf&quot;&gt;https://arxiv.org/pdf/1910.04098.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/louiskirsch/metagenrl&quot;&gt;https://github.com/louiskirsch/metagenrl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;discovering-reinforcement-learning-algorithms&quot;&gt;Discovering Reinforcement Learning Algorithms&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:2007.08794 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2007.08794&quot;&gt;https://arxiv.org/pdf/2007.08794&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-based-adversarial-meta-reinforcement-learning&quot;&gt;Model-based Adversarial Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2020&lt;/li&gt;
  &lt;li&gt;method: AdMRL&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2006.08875v2.pdf&quot;&gt;https://arxiv.org/pdf/2006.08875v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/LinZichuan/AdMRL&quot;&gt;https://github.com/LinZichuan/AdMRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-based-meta-rl&quot;&gt;optimization-based meta-RL&lt;/h3&gt;
&lt;h4 id=&quot;model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks&quot;&gt;&lt;a href=&quot;http://localhost:4000/2021/06/03/MAML.html&quot;&gt;Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML 2017&lt;/li&gt;
  &lt;li&gt;method: MAML-RL&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;2D navigation (rllab)&lt;/li&gt;
      &lt;li&gt;locomotion (rllab)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1703.03400.pdf&quot;&gt;https://arxiv.org/pdf/1703.03400.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/cbfinn/maml_rl&quot;&gt;https://github.com/cbfinn/maml_rl(tensorflow)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/tristandeleu/pytorch-maml-rl&quot;&gt;https://github.com/tristandeleu/pytorch-maml-rl&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;on-first-order-meta-learning-algorithms&quot;&gt;&lt;a href=&quot;http://localhost:4000/2021/06/04/Reptile.html&quot;&gt;On First-Order Meta-Learning Algorithms&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1803.02999 2018&lt;/li&gt;
  &lt;li&gt;method: Reptile&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;few-shot image classification
        &lt;ul&gt;
          &lt;li&gt;mini-ImageNet&lt;/li&gt;
          &lt;li&gt;Omniglot&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.02999.pdf&quot;&gt;https://arxiv.org/pdf/1803.02999.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/openai/supervised-reptile&quot;&gt;https://github.com/openai/supervised-reptile&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;https://openai.com/blog/reptile/&lt;/li&gt;
      &lt;li&gt;https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#reptile&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-of-structured-exploration-strategies&quot;&gt;Meta-Reinforcement Learning of Structured Exploration Strategies&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2018&lt;/li&gt;
  &lt;li&gt;method: MAESN (model agnostic exploration with structured noise)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;robotic locomotion (rllab)&lt;/li&gt;
      &lt;li&gt;object manipulation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1802.07245.pdf&quot;&gt;https://arxiv.org/pdf/1802.07245.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/russellmendonca/maesn_suite&quot;&gt;https://github.com/russellmendonca/maesn_suite&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/63072582&quot;&gt;https://zhuanlan.zhihu.com/p/63072582&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;some-considerations-on-learning-to-explore-via-meta-reinforcement-learning&quot;&gt;Some Considerations on Learning to Explore via Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2018&lt;/li&gt;
  &lt;li&gt;method:
    &lt;ul&gt;
      &lt;li&gt;E-MAML(optimization-based)&lt;/li&gt;
      &lt;li&gt;E-$RL^2$(model-based)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/bstadie/krazyworld&quot;&gt;Krazy World&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.01118v2.pdf&quot;&gt;https://arxiv.org/pdf/1803.01118v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/geyang/e-maml&quot;&gt;https://github.com/geyang/e-maml&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;promp-proximal-meta-policy-search&quot;&gt;ProMP: Proximal Meta-Policy Search&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method: ProMP&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;locomotion(gym &amp;amp; Mujoco)
        &lt;ul&gt;
          &lt;li&gt;HalfCheetahFwdBack&lt;/li&gt;
          &lt;li&gt;AntRandDir&lt;/li&gt;
          &lt;li&gt;HopperRandParams&lt;/li&gt;
          &lt;li&gt;WalkerFwdBack&lt;/li&gt;
          &lt;li&gt;HumanoidRandDir&lt;/li&gt;
          &lt;li&gt;WalkerRandParams&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=SkxXCi0qFX&quot;&gt;https://openreview.net/pdf?id=SkxXCi0qFX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/jonasrothfuss/promp&quot;&gt;https://github.com/jonasrothfuss/promp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;efficient-off-policy-meta-reinforcement-learning-via-probabilistic-context-variables&quot;&gt;Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML2019&lt;/li&gt;
  &lt;li&gt;method: PEARL (probabilistic embeddings for actor-critic RL)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;robotic locomotion (MuJoCo, MuJoCo200, MuJoCu133)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1903.08254.pdf&quot;&gt;https://arxiv.org/pdf/1903.08254.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/katerakelly/oyster&quot;&gt;https://github.com/katerakelly/oyster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://bair.berkeley.edu/blog/2019/06/10/pearl/&quot;&gt;https://bair.berkeley.edu/blog/2019/06/10/pearl/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-to-adapt-in-dynamic-real-world-environments-through-meta-reinforcement-learning&quot;&gt;Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method:
    &lt;ul&gt;
      &lt;li&gt;Model-Based Meta-Reinforcement Learning (train time)&lt;/li&gt;
      &lt;li&gt;Online Model Adaptation (test time)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;Mujoco
        &lt;ul&gt;
          &lt;li&gt;half cheetah:disabled joint, sloped terrain, pier&lt;/li&gt;
          &lt;li&gt;Ant: crippled leg&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;real world robot&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.11347v6.pdf&quot;&gt;https://arxiv.org/pdf/1803.11347v6.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/iclavera/learning_to_adapt&quot;&gt;https://github.com/iclavera/learning_to_adapt&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://sites.google.com/berkeley.edu/metaadaptivecontrol&quot;&gt;https://sites.google.com/berkeley.edu/metaadaptivecontrol&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-to-learn-how-to-learn-self-adaptive-visual-navigation-using-meta-learning&quot;&gt;&lt;a href=&quot;http://localhost:4000/2021/06/06/savn.html&quot;&gt;Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: CVPR 2019&lt;/li&gt;
  &lt;li&gt;method: savn&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;3D navigation (ai2thor)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1812.00971v2.pdf&quot;&gt;https://arxiv.org/pdf/1812.00971v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/allenai/savn&quot;&gt;https://github.com/allenai/savn&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/154184867&quot;&gt;https://zhuanlan.zhihu.com/p/154184867&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-q-learning&quot;&gt;Meta-Q-Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1910.00125.pdf&quot;&gt;https://arxiv.org/pdf/1910.00125.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;decoupling-exploration-and-exploitation-for-meta-reinforcement-learning-without-sacrifices&quot;&gt;&lt;a href=&quot;http://localhost:4000/2021/06/05/dream.html&quot;&gt;Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	ICML 2021&lt;/li&gt;
  &lt;li&gt;method: Dream&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/maximecb/gym-miniworld&quot;&gt;gym-miniworld 3D navigation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02790v2.pdf&quot;&gt;https://arxiv.org/pdf/2008.02790v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/ezliu/dream&quot;&gt;https://github.com/ezliu/dream&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ezliu.github.io/dream/&quot;&gt;https://ezliu.github.io/dream/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-learning-via-learned-loss&quot;&gt;Meta Learning via Learned Loss&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICPR 2021&lt;/li&gt;
  &lt;li&gt;method: ML^3&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1906.05374.pdf&quot;&gt;https://arxiv.org/pdf/1906.05374.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;未分类&quot;&gt;未分类&lt;/h3&gt;
&lt;h4 id=&quot;alchemy-a-structured-task-distribution-for-meta-reinforcement-learning&quot;&gt;Alchemy: A structured task distribution for meta-reinforcement learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: arXiv:2102.02926 preprint 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2102.02926v1.pdf&quot;&gt;https://arxiv.org/pdf/2102.02926v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/deepmind/dm_alchemy&quot;&gt;https://github.com/deepmind/dm_alchemy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://deepmind.com/research/publications/alchemy&quot;&gt;https://deepmind.com/research/publications/alchemy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-robust-state-abstractions-for-hidden-parameter-block-mdps&quot;&gt;Learning Robust State Abstractions for Hidden-Parameter Block MDPs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2007.07206v4.pdf&quot;&gt;https://arxiv.org/pdf/2007.07206v4.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-as-task-inference&quot;&gt;Meta reinforcement learning as task inference&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1905.06424 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1905.06424v2.pdf&quot;&gt;https://arxiv.org/pdf/1905.06424v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meld-meta-reinforcement-learning-from-images-via-latent-state-models&quot;&gt;MELD: Meta-Reinforcement Learning from Images via Latent State Models&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: CoRL 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.13957v2.pdf&quot;&gt;https://arxiv.org/pdf/2010.13957v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/tonyzhaozh/meld&quot;&gt;https://github.com/tonyzhaozh/meld&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://sites.google.com/view/meld-lsm&quot;&gt;https://sites.google.com/view/meld-lsm&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-with-task-embedding-and-shared-policy&quot;&gt;Meta Reinforcement Learning with Task Embedding and Shared Policy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: IJCAI 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1905.06527v3.pdf&quot;&gt;https://arxiv.org/pdf/1905.06527v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/llan-ml/tesp&quot;&gt;https://github.com/llan-ml/tesp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;fast-adaptive-task-offloading-in-edge-computing-based-on-meta-reinforcement-learning&quot;&gt;Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ITPDS 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02033v5.pdf&quot;&gt;https://arxiv.org/pdf/2008.02033v5.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/linkpark/metarl-offloading&quot;&gt;https://github.com/linkpark/metarl-offloading&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-associative-inference-using-fast-weight-memory&quot;&gt;Learning Associative Inference Using Fast Weight Memory&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2011.07831v2.pdf&quot;&gt;https://arxiv.org/pdf/2011.07831v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/ischlag/Fast-Weight-Memory-public&quot;&gt;https://github.com/ischlag/Fast-Weight-Memory-public&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;few-shot-complex-knowledge-base-question-answering-via-meta-reinforcement-learning&quot;&gt;Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: EMNLP 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.15877v1.pdf&quot;&gt;https://arxiv.org/pdf/2010.15877v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/DevinJake/MRL-CQA&quot;&gt;https://github.com/DevinJake/MRL-CQA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-with-autonomous-inference-of-subtask-dependencies&quot;&gt;Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2001.00248v2.pdf&quot;&gt;https://arxiv.org/pdf/2001.00248v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/srsohn/msgi&quot;&gt;https://github.com/srsohn/msgi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning&quot;&gt;Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf&quot;&gt;http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/oxwhirl/loaded-dice&quot;&gt;https://github.com/oxwhirl/loaded-dice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;causal-reasoning-from-meta-reinforcement-learning&quot;&gt;Causal Reasoning from Meta-reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1901.08162v1.pdf&quot;&gt;https://arxiv.org/pdf/1901.08162v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/kantneel/causal-metarl&quot;&gt;https://github.com/kantneel/causal-metarl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;introducing-neuromodulation-in-deep-neural-networks-to-learn-adaptive-behaviours&quot;&gt;Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1812.09113 preprint 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1812.09113v3.pdf&quot;&gt;https://arxiv.org/pdf/1812.09113v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/nvecoven/nmd_net&quot;&gt;https://github.com/nvecoven/nmd_net&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;policy-gradient-rl-algorithms-as-directed-acyclic-graphs&quot;&gt;Policy Gradient RL Algorithms as Directed Acyclic Graphs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: 	arXiv:2012.07763 preprint 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2012.07763v2.pdf&quot;&gt;https://arxiv.org/pdf/2012.07763v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/jjgarau/DAGPolicyGradient&quot;&gt;https://github.com/jjgarau/DAGPolicyGradient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evolving-inborn-knowledge-for-fast-adaptation-in-dynamic-pomdp-problems&quot;&gt;Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: GECCO 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2004.12846v2.pdf&quot;&gt;https://arxiv.org/pdf/2004.12846v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/dlpbc/penn-a&quot;&gt;https://github.com/dlpbc/penn-a&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-based-meta-reinforcement-learning-for-flight-with-suspended-payloads&quot;&gt;Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: RA-L 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2004.11345v2.pdf&quot;&gt;https://arxiv.org/pdf/2004.11345v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/suneelbelkhale/model-based-meta-rl-for-flight&quot;&gt;https://github.com/suneelbelkhale/model-based-meta-rl-for-flight&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hierarchical-meta-reinforcement-learning-for-multi-task-environments&quot;&gt;Hierarchical Meta Reinforcement Learning for Multi-Task Environments&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=u9ax42K7ND&quot;&gt;https://openreview.net/pdf?id=u9ax42K7ND&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/MeSH-ICLR/MEtaSoftHierarchy&quot;&gt;https://github.com/MeSH-ICLR/MEtaSoftHierarchy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;modeling-and-optimization-trade-off-in-meta-learning&quot;&gt;Modeling and Optimization Trade-off in Meta-learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.12916v2.pdf&quot;&gt;https://arxiv.org/pdf/2010.12916v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/intel-isl/MetaLearningTradeoffs&quot;&gt;https://github.com/intel-isl/MetaLearningTradeoffs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-learning-of-structured-task-distributions-in-humans-and-machines&quot;&gt;Meta-Learning of Structured Task Distributions in Humans and Machines&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.02317v3.pdf&quot;&gt;https://arxiv.org/pdf/2010.02317v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/sreejank/Compositional_MetaRL&quot;&gt;https://github.com/sreejank/Compositional_MetaRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;offline-meta-learning-of-exploration&quot;&gt;Offline Meta Learning of Exploration&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:2008.02598 preprint 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02598v3.pdf&quot;&gt;https://arxiv.org/pdf/2008.02598v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/Rondorf/BOReL&quot;&gt;https://github.com/Rondorf/BOReL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-for-reliable-communication-in-thzvlc-wireless-vr-networks&quot;&gt;Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICC 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2102.12277v1.pdf&quot;&gt;https://arxiv.org/pdf/2102.12277v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/wyy0206/THzVR&quot;&gt;https://github.com/wyy0206/THzVR&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
#### 
- source: 
- method: 
- environment:
- paper link: 
- code:  
- interpretation: 

--&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">Challenges of meta-RL design a set of tasks that are interrelated find the inter-representation fast adaptation to new tasks Papers environment Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning source: PMLR 2020 method: None environment: object manipulation paper link: http://proceedings.mlr.press/v100/yu20a/yu20a.pdf code: https://github.com/rlworkgroup/metaworld interpretation: https://meta-world.github.io/ model-based meta-RL Learning to reinforcement learn source: CogSci 2017 method: deep meta-RL environment: bandit problem Two-step task Harlow experiment 3D navigation (Deepmind Lab) paper link: https://arxiv.org/pdf/1611.05763.pdf code: https://github.com/Phu-archive/Learning2RL interpretation: RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning source: ICLR 2017 method: $RL^2$ environment: multi-armed bandit problem tabular MDP 3D navigation (ViZDoom) paper link: https://arxiv.org/pdf/1611.02779.pdf code: https://github.com/mwufi/meta-rl-bandits (pytorch) https://github.com/VashishtMadhavan/rl2 (tensorflow) interpretation: https://zhuanlan.zhihu.com/p/32606591 https://openreview.net/forum?id=HkLXCE9lx Prefrontal cortex as a meta-reinforcement learning system source: Nature Neuroscience 2018 method: environment: paper link: https://www.nature.com/articles/s41593-018-0147-8 code: interpretation: A Simple Neural Attentive Meta-Learner source: ICLR 2018 method: SNAIL (simple neural attentive learner) environment: navigation robotic locomotion paper link: https://openreview.net/pdf?id=B1DmUzWAW code: https://github.com/eambutu/snail-pytorch interpretation: https://www.carsi.edu.cn/index_zh.htm PixelSNAIL: An Improved Autoregressive Generative Model source: ICML 2018 method: environment: paper link: https://arxiv.org/pdf/1712.09763v1.pdf code: https://github.com/neocxi/pixelsnail-public interpretation: Concurrent Meta Reinforcement Learning source: arXiv:1903.02710 preprint method: CMRL environment: N-Monty-Hall N-Color-Choice N-Reacher (Reacher-V2 from gym) paper link: https://arxiv.org/pdf/1903.02710v1.pdf code: interpretation: Reinforcement Learning, Fast and Slow source: Trends in Cognitive Sciences 2019 method: environment: paper link: https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0 code: interpretation: Improving Generalization in Meta Reinforcement Learning using Learned Objectives source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/1910.04098.pdf code: https://github.com/louiskirsch/metagenrl interpretation: Discovering Reinforcement Learning Algorithms source: arXiv:2007.08794 2020 method: environment: paper link: https://arxiv.org/pdf/2007.08794 code: interpretation: Model-based Adversarial Meta-Reinforcement Learning source: NeurIPS 2020 method: AdMRL environment: paper link: https://arxiv.org/pdf/2006.08875v2.pdf code: https://github.com/LinZichuan/AdMRL interpretation: optimization-based meta-RL Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks source: ICML 2017 method: MAML-RL environment: 2D navigation (rllab) locomotion (rllab) paper link: https://arxiv.org/pdf/1703.03400.pdf code: https://github.com/cbfinn/maml_rl(tensorflow) https://github.com/tristandeleu/pytorch-maml-rl interpretation: On First-Order Meta-Learning Algorithms source: arXiv:1803.02999 2018 method: Reptile environment: few-shot image classification mini-ImageNet Omniglot paper link: https://arxiv.org/pdf/1803.02999.pdf code: https://github.com/openai/supervised-reptile interpretation: https://openai.com/blog/reptile/ https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#reptile Meta-Reinforcement Learning of Structured Exploration Strategies source: NeurIPS 2018 method: MAESN (model agnostic exploration with structured noise) environment: robotic locomotion (rllab) object manipulation paper link: https://arxiv.org/pdf/1802.07245.pdf code: https://github.com/russellmendonca/maesn_suite interpretation: https://zhuanlan.zhihu.com/p/63072582 Some Considerations on Learning to Explore via Meta-Reinforcement Learning source: ICLR 2018 method: E-MAML(optimization-based) E-$RL^2$(model-based) environment: Krazy World paper link: https://arxiv.org/pdf/1803.01118v2.pdf code: https://github.com/geyang/e-maml interpretation: ProMP: Proximal Meta-Policy Search source: ICLR 2019 method: ProMP environment: locomotion(gym &amp;amp; Mujoco) HalfCheetahFwdBack AntRandDir HopperRandParams WalkerFwdBack HumanoidRandDir WalkerRandParams paper link: https://openreview.net/pdf?id=SkxXCi0qFX code: https://github.com/jonasrothfuss/promp Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables source: ICML2019 method: PEARL (probabilistic embeddings for actor-critic RL) environment: robotic locomotion (MuJoCo, MuJoCo200, MuJoCu133) paper link: https://arxiv.org/pdf/1903.08254.pdf code: https://github.com/katerakelly/oyster interpretation: https://bair.berkeley.edu/blog/2019/06/10/pearl/ Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning source: ICLR 2019 method: Model-Based Meta-Reinforcement Learning (train time) Online Model Adaptation (test time) environment: Mujoco half cheetah:disabled joint, sloped terrain, pier Ant: crippled leg real world robot paper link: https://arxiv.org/pdf/1803.11347v6.pdf code: https://github.com/iclavera/learning_to_adapt interpretation: https://sites.google.com/berkeley.edu/metaadaptivecontrol Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning source: CVPR 2019 method: savn environment: 3D navigation (ai2thor) paper link: https://arxiv.org/pdf/1812.00971v2.pdf code: https://github.com/allenai/savn interpretation: https://zhuanlan.zhihu.com/p/154184867 Meta-Q-Learning source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/1910.00125.pdf code: interpretation: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices source: ICML 2021 method: Dream environment: gym-miniworld 3D navigation paper link: https://arxiv.org/pdf/2008.02790v2.pdf code: https://github.com/ezliu/dream interpretation: https://ezliu.github.io/dream/ Meta Learning via Learned Loss source: ICPR 2021 method: ML^3 environment: paper link: https://arxiv.org/pdf/1906.05374.pdf code: interpretation: 未分类 Alchemy: A structured task distribution for meta-reinforcement learning source: arXiv:2102.02926 preprint 2021 method: environment: paper link: https://arxiv.org/pdf/2102.02926v1.pdf code: https://github.com/deepmind/dm_alchemy interpretation: https://deepmind.com/research/publications/alchemy Learning Robust State Abstractions for Hidden-Parameter Block MDPs source: ICLR2021 method: environment: paper link: https://arxiv.org/pdf/2007.07206v4.pdf code: interpretation: Meta reinforcement learning as task inference source: arXiv:1905.06424 2019 method: environment: paper link: https://arxiv.org/pdf/1905.06424v2.pdf code: interpretation: MELD: Meta-Reinforcement Learning from Images via Latent State Models source: CoRL 2020 method: environment: paper link: https://arxiv.org/pdf/2010.13957v2.pdf code: https://github.com/tonyzhaozh/meld interpretation: https://sites.google.com/view/meld-lsm Meta Reinforcement Learning with Task Embedding and Shared Policy source: IJCAI 2019 method: environment: paper link: https://arxiv.org/pdf/1905.06527v3.pdf code: https://github.com/llan-ml/tesp interpretation: Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning source: ITPDS 2020 method: environment: paper link: https://arxiv.org/pdf/2008.02033v5.pdf code: https://github.com/linkpark/metarl-offloading interpretation: Learning Associative Inference Using Fast Weight Memory source: ICLR 2021 method: environment: paper link: https://arxiv.org/pdf/2011.07831v2.pdf code: https://github.com/ischlag/Fast-Weight-Memory-public interpretation: Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning source: EMNLP 2020 method: environment: paper link: https://arxiv.org/pdf/2010.15877v1.pdf code: https://github.com/DevinJake/MRL-CQA interpretation: Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/2001.00248v2.pdf code: https://github.com/srsohn/msgi interpretation: Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning source: NeurIPS 2019 method: environment: paper link: http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf code: https://github.com/oxwhirl/loaded-dice interpretation: Causal Reasoning from Meta-reinforcement Learning source: ICLR 2019 method: environment: paper link: https://arxiv.org/pdf/1901.08162v1.pdf code: https://github.com/kantneel/causal-metarl interpretation: Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours source: arXiv:1812.09113 preprint 2019 method: environment: paper link: https://arxiv.org/pdf/1812.09113v3.pdf code: https://github.com/nvecoven/nmd_net interpretation: Policy Gradient RL Algorithms as Directed Acyclic Graphs source: arXiv:2012.07763 preprint 2020 method: environment: paper link: https://arxiv.org/pdf/2012.07763v2.pdf code: https://github.com/jjgarau/DAGPolicyGradient interpretation: Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems source: GECCO 2020 method: environment: paper link: https://arxiv.org/pdf/2004.12846v2.pdf code: https://github.com/dlpbc/penn-a interpretation: Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads source: RA-L 2021 method: environment: paper link: https://arxiv.org/pdf/2004.11345v2.pdf code: https://github.com/suneelbelkhale/model-based-meta-rl-for-flight interpretation: Hierarchical Meta Reinforcement Learning for Multi-Task Environments source: ICLR 2021 method: environment: paper link: https://openreview.net/pdf?id=u9ax42K7ND code: https://github.com/MeSH-ICLR/MEtaSoftHierarchy interpretation: Modeling and Optimization Trade-off in Meta-learning source: NeurIPS 2020 method: environment: paper link: https://arxiv.org/pdf/2010.12916v2.pdf code: https://github.com/intel-isl/MetaLearningTradeoffs interpretation: Meta-Learning of Structured Task Distributions in Humans and Machines source: ICLR 2021 method: environment: paper link: https://arxiv.org/pdf/2010.02317v3.pdf code: https://github.com/sreejank/Compositional_MetaRL interpretation: Offline Meta Learning of Exploration source: arXiv:2008.02598 preprint 2021 method: environment: paper link: https://arxiv.org/pdf/2008.02598v3.pdf code: https://github.com/Rondorf/BOReL interpretation: Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks source: ICC 2021 method: environment: paper link: https://arxiv.org/pdf/2102.12277v1.pdf code: https://github.com/wyy0206/THzVR interpretation:</summary></entry><entry><title type="html">Tips on How To Succeed in an ECE Major(如何在电子和计算机工程专业取得成功？)</title><link href="http://localhost:4000/2020/12/23/success.html" rel="alternate" type="text/html" title="Tips on How To Succeed in an ECE Major(如何在电子和计算机工程专业取得成功？)" /><published>2020-12-23T00:00:00+08:00</published><updated>2020-12-23T00:00:00+08:00</updated><id>http://localhost:4000/2020/12/23/success</id><content type="html" xml:base="http://localhost:4000/2020/12/23/success.html">&lt;blockquote&gt;
  &lt;p&gt;转自Reddit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;给还在迷茫的自动化、计算机、电子系的本科生。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tips on How To Succeed in an ECE Major (or any degree)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do the homework.&lt;/strong&gt; Even if it’s not required to turn in. It’s the only way to learn this ECE stuff. Don’t cheat on the homework, because when the exam comes around the lack of actual practice will show and you will do poorly. This is how a lot of my friends failed ECE classes. They just copied solutions for the homeworks, got all A’s on the homeworks, then failed the exams and the class. The solutions can be helpful if you use them to work through problems and learn, but they can be very hurtful if you use them wrong.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Work the hard problems.&lt;/strong&gt; Once you get really good at a certain type of problem, start working on a different type of problem. It’s easy to keep doing the problems you are good at and skip the hard ones. This is the opposite of what you should be doing! Work on the hard problems so you get practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treat the school day like a work day.&lt;/strong&gt; Regardless of when your classes are, get to school at 8:00 AM, and stay until 5:00 PM. If you have an exam or something the next day, then stay longer. Work hard during that time to finish all your homework, studying, office hours, emails, etc. That way your evenings could possibly be free to get rest or have some fun!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to the professor’s office hours.&lt;/strong&gt; They are different people in their office than in front of class. They will answer your questions in a direct way. You will need letters of recommendation at some point in your life (applying for grad school, scholarships, jobs) and it is part of the professor’s job to write letters of rec for students. If you know them even a little bit, like by going to a few office hours, then they will gladly write you a great letter of rec later in life.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to the TA office hours.&lt;/strong&gt; They are usually awesome and will help you work through problems, and sometimes give you the solutions. They are students, too, so they understand what you are dealing with and may be able to explain things in a different way than the professors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make the circuit on a breadboard or in a simulator.&lt;/strong&gt; Nothing beats learning the concepts then actually putting them into practice. It is infinitely helpful to make the circuits from homework or the textbook examples with an actual breadboard. Remember to engineer hardware you have to understand the hardware, and the best way to do that is play with the actual hardware. Components are cheap on Amazon, or there are some free circuit simulators if you want. I personally used everycircuit even though it is paid, it is worth it for the visualizations. Another browser based one is PartSim. Here is a list of free simulators.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All-night cramming study sessions are hurtful.&lt;/strong&gt; They will not help you in an engineering degree. Don’t do them. You will need rest before an exam so you can think critically and your brain can have better recall. Sleep is key! To achieve no all-nighters, you have to start working on assignments early, and start studying early. Spreading out the studying over multiple days is helpful if you can do it early enough.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use the campus tutoring resources.&lt;/strong&gt; A lot of schools or departments offer free/cheap tutoring services. All of them will have some sort of resources. Ask your department counselor or admin person, they will point you in the right direction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Awesome with no effort” doesn’t happen in college.&lt;/strong&gt; You will have to make a deliberate effort to learn the material. You will have to make time to study and do homework. You will work your ass off. In high school or other degrees it is possible to rely on your smarts and just cruise through. That is not possible anymore, you gotta put in work to do well. Just being smart doesn’t count anymore, you have to have discipline and a work ethic. Luckily, those can be learned!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start working on assignments based on the assigned date, not the due date.&lt;/strong&gt; Don’t treat due dates like you did in high school, etc. Instead of planning assignments from the due date backwards, start doing work from the assigned date forwards. This is a big one! For example: If a homework is assigned on 11/5 but isn’t due until 11/20, don’t plan on doing the homework on 11/18. Instead, plan on doing the homework on 11/6. You could even trick yourself by writing down that the homework is due a few days before it actually is. That way you are done early and aren’t stressed out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start studying for exams a few (2-3) weeks before they come.&lt;/strong&gt; If you start studying the week before you will be stressed, not cover enough material, and struggle. Help yourself out and start early. It feels amazing when the exam is a day or two away and you already are done studying for it. Now any more studying is just an easy review, which makes you feel more confident, which makes you do better on the exam.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to each lecture.&lt;/strong&gt; If learning and excelling isn’t enough motivation, then think of the cost. You are throwing hundreds of dollars into the trash every time you skip a class. Just go. Even if you are sleepy or don’t want to. It’s absolutely critical to get in this habit.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Take notes by hand on paper, sit in the front rows, and don’t be afraid to ask questions.&lt;/strong&gt; You are at school, asking questions is part of learning! If the professor thought you didn’t have any questions, then what would be the purpose of school? Ask questions!(注：ipad记笔记体验也不错:))&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Take notes in your own words.&lt;/strong&gt; Simply copying down what the professor says word for word is fine for reference, but is not that helpful for learning. The textbook can be your reference. It is better to take notes in your own words, it forces you to process the information and write it in a new way, which increases retention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do the reading before the lecture.&lt;/strong&gt; I personally only did this a few times, but each time I did I felt like a freaking genius during the lecture. Even if I didn’t really get what I was reading, just being exposed to the terms and concepts made the lecture so much more effective.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to review your notes after the lecture.&lt;/strong&gt; What good are notes if you don’t look at them again?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find a good quiet place to study and make it your regular spot.&lt;/strong&gt; I prefer top floors of libraries or basements. Headphones are great with some homework music on. Gotta find a zone that works for you and get into it. Sometimes music is distracting, and white noise would be better. I prefer brown noise. Here’s a website that does both!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find a good time of day to study.&lt;/strong&gt; If you like studying at certain times of the day, then plan those times for studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make the most of your study time.&lt;/strong&gt; It’s easy to sit at a desk for 8 hours straight then say “I just studied for 8 hours.” In my experience, that’s not real studying. Most likely half or two thirds of that time was actually spent studying. Get the studying done with, then move on with your day, otherwise you will get burned out. Sometimes forcing small breaks and chunking your study time helps, like in the Pomodoro technique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Study one class at a time.&lt;/strong&gt; Context switching is an expensive mental operation. Meaning it wastes time to switch between classes. When studying or doing homework try to finish one thing first, then move on. It will feel good to get something off your plate as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ditch the laptop and phone when studying if you can.&lt;/strong&gt; Try to study with just a textbook, calculator, notebook, and pencil. It’s easy to get overwhelmed with homework and just browse reddit/facebook/internet for hours instead. Or play computer games. The biggest thing that ever helped me was studying without internet enabled electronic devices. It also helps to put a timer on your internet usage, so it makes your quick Google searches more urgent and less prone to wandering.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to find a study group with 2-5 students.&lt;/strong&gt; It really helps to have different people to study with, because everyone sees things slightly different. Plus it’s harder to slack off and play computer games when you have people sitting next to you. Too many people in a study group means that it becomes a social event, and no work gets done. I was very shy and didn’t get a study group until my last year of undergrad. It was just 2 people that I randomly asked “hey wanna study”. They will help you and you will help them and you will all be smarter. And you will feel social. Win win!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Teach someone else the material.&lt;/strong&gt; Saying the concepts out loud and forcing your brain to make sense of what you’re saying is a great way to learn the material. Yes, teaching helps you learn, it’s proven scientifically! You can do this with a classmate, friend, random person, or a rubber duck.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn the material from a different resource on your own.&lt;/strong&gt; Watch YouTube videos, MIT OCW videos, read Wikipedia articles, look for animations, do anything to learn the material in a different way. I highly recommend visualizations of circuits, EM fields, computer architecture, etc. Any of the complex things that will be learned in an ECE degree can be much more readily understood with a good diagram or animation. Here’s some personal favorites: Falstad circuit examples and an amazing Electric Circuits YouTube playlist&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use tools to supplement learning, but don’t abuse them.&lt;/strong&gt; Chegg step-by-step solutions, Wolfram Mathematica, etc. are very useful tools that can help you understand how to work through problems. Just remember that they are tools you can use to increase your knowledge. Don’t abuse them, or it will show on your exams and in your job interviews.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Have pride in what you are turning in.&lt;/strong&gt; Make your circuits look nice, check your spelling, format things nicely. It makes you feel good and makes you put more effort into the whole assignment, which ultimately gives you a better grade.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Understanding the foundations is a key to success.&lt;/strong&gt; This isn’t simply a memorization degree. You must really understand what’s going on at the basic physical level. You will excel in all the other things if you can get a handle on the basics (ie. Kirchhoff’s laws, Ohm’s laws, EM fields, basic C programming, calculus, statistics).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do something non-engineering to keep yourself sane.&lt;/strong&gt; Sketch, read sci-fi books, meditate, go on walks, practice piano, play Frisbee, something to balance out all the left brain activity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The only way to learn is to do problems.&lt;/strong&gt; Certain degrees can be got by rote memorization. Engineering degrees can only be got by critical thinking and actually doing. Do the problems. Do them multiple ways. Make the circuit on a breadboard or in a simulator. Use your hands, use your brain!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find practice problems, homework from other schools/classes, textbook solutions, etc.&lt;/strong&gt; These are excellent resources for doing problems and learning. Be careful with solutions though!! It is easy to feel confident about a problem when doing it with solutions. That same problem on an exam will feel impossible because you don’t have solutions to look at. I like to use solutions to get me unstuck or to confirm I did the right step.(注：非常重要！！！国内的课程无论从lecture的内容丰富度、新鲜度还是实验的完善度、难度远没有国外的如MIT、伯克利等学校完善，学会利用国外学校的免费教育资源！)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google for previous classes to find old lectures, problems, etc.&lt;/strong&gt; If you are taking ECE 165 at UCSB, just google “ece 165 ucsb” or “ece 165 ucsb 2015” to find previous quarter’s class websites. They usually have old homeworks, lectures, solutions, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When studying for exams, make equation cheat sheets.&lt;/strong&gt; Distill all the complex equations, terms, conversions, etc. onto 1-2 pages. Do this even if your class does not allow them during the exam, just making them is a great study practice as it forces you to think about each important thing before an exam. Plus it’s a great resource later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make a “roadmap” before studying.&lt;/strong&gt; Usually this can be found in the syllabus. Basically you want to make a list of the topics you will learn so that you can quantify the amount of work you will be doing during the course. This makes your life much easier when studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make a “toolbox” after/during studying.&lt;/strong&gt; I like to take the key concepts from each chapter or topic and distill it to a single page of graphs, circuits, equations, terms, etc. This becomes a great reference and is a good exercise to parse the large amount of information you are learning into the important bits. Most courses are a bit like gold mining, you have to sift through and find the critical nuggets of information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At some point you will do poorly on an exam, homework, class, etc.&lt;/strong&gt; Don’t worry! Just take it as a sign that you need to devote more time and effort to the material. You know what they call medical students who get C’s? Doctor. You should try your best and take pride in your performance, but if you get hit with a bad grade don’t get down, just study harder next time!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use a planner or homework tracker to keep yourself on schedule.&lt;/strong&gt; There are a lot of assignments and you would be crazy to try to memorize it all. Just get a nice planner, or use Google Calendar, whatever. Schedule your classes, homework, and (importantly) time for studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get connected on LinkedIn.&lt;/strong&gt; Connect with your peers and professor on LinkedIn! It’s a very useful tool to stay connected in a professional environment and the people hiring at companies always look up applicants on LinkedIn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn to teach yourself.&lt;/strong&gt; A degree in ECE is partly about learning the specific material, but it is mostly about learning critical thinking and how to teach yourself. These are the two big things that you will use for the rest of your life. At your future job you will have tools that do a lot of the calculations and work for you, but you must know how to teach yourself to use them and to think critically about the applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remember, this is a hard major.&lt;/strong&gt; So you gotta work hard. It will be a stressful challenge, but you can make your life easier by being smart about how you do the work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remember, this is a rewarding major.&lt;/strong&gt; ECE will pay off! Jobs are often exciting and interesting, and pay well. Here’s some statistics and information: Glassdoor salary estimates, Bureau of Labor Statistics employment stats and occupational outlook&lt;/p&gt;

&lt;p&gt;评论区里的补充：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to your career fairs on campus each semester if they happen that frequently.&lt;/strong&gt; Try talking with each of the representatives and find out what kind of classes and what kind of work they are doing. You get to practice your communication skills and also learn more about the industry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;textbooks are expensive.&lt;/strong&gt; if you are broke and are savvy with sketchy parts of the web, you can find most of the textbooks as PDF’s online using the guide at the link below. Virus scan everything and be smart about what you download. Piracy is bad, mmkay, buyer beware!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原始链接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&quot;reddit-card&quot; data-card-created=&quot;1608721895&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/ECE/comments/52rtx6/tips_on_how_to_succeed_in_an_ece_major/&quot;&gt;Tips on How To Succeed in an ECE Major&lt;/a&gt; from &lt;a href=&quot;http://www.reddit.com/r/ECE&quot;&gt;r/ECE&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//embed.redditmedia.com/widgets/platform.js&quot; charset=&quot;UTF-8&quot;&gt;&lt;/script&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="随感" /><summary type="html">转自Reddit. 给还在迷茫的自动化、计算机、电子系的本科生。 Tips on How To Succeed in an ECE Major (or any degree) Do the homework. Even if it’s not required to turn in. It’s the only way to learn this ECE stuff. Don’t cheat on the homework, because when the exam comes around the lack of actual practice will show and you will do poorly. This is how a lot of my friends failed ECE classes. They just copied solutions for the homeworks, got all A’s on the homeworks, then failed the exams and the class. The solutions can be helpful if you use them to work through problems and learn, but they can be very hurtful if you use them wrong. Work the hard problems. Once you get really good at a certain type of problem, start working on a different type of problem. It’s easy to keep doing the problems you are good at and skip the hard ones. This is the opposite of what you should be doing! Work on the hard problems so you get practice. Treat the school day like a work day. Regardless of when your classes are, get to school at 8:00 AM, and stay until 5:00 PM. If you have an exam or something the next day, then stay longer. Work hard during that time to finish all your homework, studying, office hours, emails, etc. That way your evenings could possibly be free to get rest or have some fun! Go to the professor’s office hours. They are different people in their office than in front of class. They will answer your questions in a direct way. You will need letters of recommendation at some point in your life (applying for grad school, scholarships, jobs) and it is part of the professor’s job to write letters of rec for students. If you know them even a little bit, like by going to a few office hours, then they will gladly write you a great letter of rec later in life. Go to the TA office hours. They are usually awesome and will help you work through problems, and sometimes give you the solutions. They are students, too, so they understand what you are dealing with and may be able to explain things in a different way than the professors. Make the circuit on a breadboard or in a simulator. Nothing beats learning the concepts then actually putting them into practice. It is infinitely helpful to make the circuits from homework or the textbook examples with an actual breadboard. Remember to engineer hardware you have to understand the hardware, and the best way to do that is play with the actual hardware. Components are cheap on Amazon, or there are some free circuit simulators if you want. I personally used everycircuit even though it is paid, it is worth it for the visualizations. Another browser based one is PartSim. Here is a list of free simulators. All-night cramming study sessions are hurtful. They will not help you in an engineering degree. Don’t do them. You will need rest before an exam so you can think critically and your brain can have better recall. Sleep is key! To achieve no all-nighters, you have to start working on assignments early, and start studying early. Spreading out the studying over multiple days is helpful if you can do it early enough. Use the campus tutoring resources. A lot of schools or departments offer free/cheap tutoring services. All of them will have some sort of resources. Ask your department counselor or admin person, they will point you in the right direction. “Awesome with no effort” doesn’t happen in college. You will have to make a deliberate effort to learn the material. You will have to make time to study and do homework. You will work your ass off. In high school or other degrees it is possible to rely on your smarts and just cruise through. That is not possible anymore, you gotta put in work to do well. Just being smart doesn’t count anymore, you have to have discipline and a work ethic. Luckily, those can be learned! Start working on assignments based on the assigned date, not the due date. Don’t treat due dates like you did in high school, etc. Instead of planning assignments from the due date backwards, start doing work from the assigned date forwards. This is a big one! For example: If a homework is assigned on 11/5 but isn’t due until 11/20, don’t plan on doing the homework on 11/18. Instead, plan on doing the homework on 11/6. You could even trick yourself by writing down that the homework is due a few days before it actually is. That way you are done early and aren’t stressed out. Start studying for exams a few (2-3) weeks before they come. If you start studying the week before you will be stressed, not cover enough material, and struggle. Help yourself out and start early. It feels amazing when the exam is a day or two away and you already are done studying for it. Now any more studying is just an easy review, which makes you feel more confident, which makes you do better on the exam. Go to each lecture. If learning and excelling isn’t enough motivation, then think of the cost. You are throwing hundreds of dollars into the trash every time you skip a class. Just go. Even if you are sleepy or don’t want to. It’s absolutely critical to get in this habit. Take notes by hand on paper, sit in the front rows, and don’t be afraid to ask questions. You are at school, asking questions is part of learning! If the professor thought you didn’t have any questions, then what would be the purpose of school? Ask questions!(注：ipad记笔记体验也不错:)) Take notes in your own words. Simply copying down what the professor says word for word is fine for reference, but is not that helpful for learning. The textbook can be your reference. It is better to take notes in your own words, it forces you to process the information and write it in a new way, which increases retention. Do the reading before the lecture. I personally only did this a few times, but each time I did I felt like a freaking genius during the lecture. Even if I didn’t really get what I was reading, just being exposed to the terms and concepts made the lecture so much more effective. Try to review your notes after the lecture. What good are notes if you don’t look at them again? Find a good quiet place to study and make it your regular spot. I prefer top floors of libraries or basements. Headphones are great with some homework music on. Gotta find a zone that works for you and get into it. Sometimes music is distracting, and white noise would be better. I prefer brown noise. Here’s a website that does both! Find a good time of day to study. If you like studying at certain times of the day, then plan those times for studying. Make the most of your study time. It’s easy to sit at a desk for 8 hours straight then say “I just studied for 8 hours.” In my experience, that’s not real studying. Most likely half or two thirds of that time was actually spent studying. Get the studying done with, then move on with your day, otherwise you will get burned out. Sometimes forcing small breaks and chunking your study time helps, like in the Pomodoro technique. Study one class at a time. Context switching is an expensive mental operation. Meaning it wastes time to switch between classes. When studying or doing homework try to finish one thing first, then move on. It will feel good to get something off your plate as well. Ditch the laptop and phone when studying if you can. Try to study with just a textbook, calculator, notebook, and pencil. It’s easy to get overwhelmed with homework and just browse reddit/facebook/internet for hours instead. Or play computer games. The biggest thing that ever helped me was studying without internet enabled electronic devices. It also helps to put a timer on your internet usage, so it makes your quick Google searches more urgent and less prone to wandering. Try to find a study group with 2-5 students. It really helps to have different people to study with, because everyone sees things slightly different. Plus it’s harder to slack off and play computer games when you have people sitting next to you. Too many people in a study group means that it becomes a social event, and no work gets done. I was very shy and didn’t get a study group until my last year of undergrad. It was just 2 people that I randomly asked “hey wanna study”. They will help you and you will help them and you will all be smarter. And you will feel social. Win win! Teach someone else the material. Saying the concepts out loud and forcing your brain to make sense of what you’re saying is a great way to learn the material. Yes, teaching helps you learn, it’s proven scientifically! You can do this with a classmate, friend, random person, or a rubber duck. Learn the material from a different resource on your own. Watch YouTube videos, MIT OCW videos, read Wikipedia articles, look for animations, do anything to learn the material in a different way. I highly recommend visualizations of circuits, EM fields, computer architecture, etc. Any of the complex things that will be learned in an ECE degree can be much more readily understood with a good diagram or animation. Here’s some personal favorites: Falstad circuit examples and an amazing Electric Circuits YouTube playlist Use tools to supplement learning, but don’t abuse them. Chegg step-by-step solutions, Wolfram Mathematica, etc. are very useful tools that can help you understand how to work through problems. Just remember that they are tools you can use to increase your knowledge. Don’t abuse them, or it will show on your exams and in your job interviews. Have pride in what you are turning in. Make your circuits look nice, check your spelling, format things nicely. It makes you feel good and makes you put more effort into the whole assignment, which ultimately gives you a better grade. Understanding the foundations is a key to success. This isn’t simply a memorization degree. You must really understand what’s going on at the basic physical level. You will excel in all the other things if you can get a handle on the basics (ie. Kirchhoff’s laws, Ohm’s laws, EM fields, basic C programming, calculus, statistics). Do something non-engineering to keep yourself sane. Sketch, read sci-fi books, meditate, go on walks, practice piano, play Frisbee, something to balance out all the left brain activity. The only way to learn is to do problems. Certain degrees can be got by rote memorization. Engineering degrees can only be got by critical thinking and actually doing. Do the problems. Do them multiple ways. Make the circuit on a breadboard or in a simulator. Use your hands, use your brain! Find practice problems, homework from other schools/classes, textbook solutions, etc. These are excellent resources for doing problems and learning. Be careful with solutions though!! It is easy to feel confident about a problem when doing it with solutions. That same problem on an exam will feel impossible because you don’t have solutions to look at. I like to use solutions to get me unstuck or to confirm I did the right step.(注：非常重要！！！国内的课程无论从lecture的内容丰富度、新鲜度还是实验的完善度、难度远没有国外的如MIT、伯克利等学校完善，学会利用国外学校的免费教育资源！) Google for previous classes to find old lectures, problems, etc. If you are taking ECE 165 at UCSB, just google “ece 165 ucsb” or “ece 165 ucsb 2015” to find previous quarter’s class websites. They usually have old homeworks, lectures, solutions, etc. When studying for exams, make equation cheat sheets. Distill all the complex equations, terms, conversions, etc. onto 1-2 pages. Do this even if your class does not allow them during the exam, just making them is a great study practice as it forces you to think about each important thing before an exam. Plus it’s a great resource later. Make a “roadmap” before studying. Usually this can be found in the syllabus. Basically you want to make a list of the topics you will learn so that you can quantify the amount of work you will be doing during the course. This makes your life much easier when studying. Make a “toolbox” after/during studying. I like to take the key concepts from each chapter or topic and distill it to a single page of graphs, circuits, equations, terms, etc. This becomes a great reference and is a good exercise to parse the large amount of information you are learning into the important bits. Most courses are a bit like gold mining, you have to sift through and find the critical nuggets of information. At some point you will do poorly on an exam, homework, class, etc. Don’t worry! Just take it as a sign that you need to devote more time and effort to the material. You know what they call medical students who get C’s? Doctor. You should try your best and take pride in your performance, but if you get hit with a bad grade don’t get down, just study harder next time! Use a planner or homework tracker to keep yourself on schedule. There are a lot of assignments and you would be crazy to try to memorize it all. Just get a nice planner, or use Google Calendar, whatever. Schedule your classes, homework, and (importantly) time for studying. Get connected on LinkedIn. Connect with your peers and professor on LinkedIn! It’s a very useful tool to stay connected in a professional environment and the people hiring at companies always look up applicants on LinkedIn. Learn to teach yourself. A degree in ECE is partly about learning the specific material, but it is mostly about learning critical thinking and how to teach yourself. These are the two big things that you will use for the rest of your life. At your future job you will have tools that do a lot of the calculations and work for you, but you must know how to teach yourself to use them and to think critically about the applications. Remember, this is a hard major. So you gotta work hard. It will be a stressful challenge, but you can make your life easier by being smart about how you do the work. Remember, this is a rewarding major. ECE will pay off! Jobs are often exciting and interesting, and pay well. Here’s some statistics and information: Glassdoor salary estimates, Bureau of Labor Statistics employment stats and occupational outlook 评论区里的补充： Go to your career fairs on campus each semester if they happen that frequently. Try talking with each of the representatives and find out what kind of classes and what kind of work they are doing. You get to practice your communication skills and also learn more about the industry. textbooks are expensive. if you are broke and are savvy with sketchy parts of the web, you can find most of the textbooks as PDF’s online using the guide at the link below. Virus scan everything and be smart about what you download. Piracy is bad, mmkay, buyer beware! 原始链接 Tips on How To Succeed in an ECE Major from r/ECE</summary></entry><entry><title type="html">cuFAST:基于GPU的FAST算法加速</title><link href="http://localhost:4000/2020/12/19/pdf-cufast.html" rel="alternate" type="text/html" title="cuFAST:基于GPU的FAST算法加速" /><published>2020-12-19T00:00:00+08:00</published><updated>2020-12-19T00:00:00+08:00</updated><id>http://localhost:4000/2020/12/19/pdf-cufast</id><content type="html" xml:base="http://localhost:4000/2020/12/19/pdf-cufast.html">&lt;center&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:718px; height:700px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;/web/viewer.html?file=/pdf/CAC2020.pdf&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:500px; height:100px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/tianyma/tianyma.github.io/blob/main/pdf/CAC2020.pdf&quot; class=&quot;image fit&quot; &gt;&lt;img src=&quot;images/marr_pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt; --&gt;
&lt;a href=&quot;/pdf.js/web/viewer.html?file=/pdf/cufast.pdf&quot;&gt;Click to view this blog.&lt;/a&gt;

&lt;!-- &lt;object data=&quot;/pdf/CAC2020.pdf&quot; width=&quot;1000&quot; height=&quot;1000&quot; type='application/pdf'/&gt; --&gt;
&lt;/center&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="cuda" /><summary type="html">Click to view this blog.</summary></entry></feed>