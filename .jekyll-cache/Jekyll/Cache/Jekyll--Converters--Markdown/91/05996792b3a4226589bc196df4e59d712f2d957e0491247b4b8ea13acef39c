I"Z_<h2 id="challenges-of-meta-rl">Challenges of meta-RL</h2>
<ul>
  <li>design a set of tasks that are interrelated</li>
  <li>find the inter-representation</li>
  <li>fast adaptation to new tasks
    <h2 id="papers">Papers</h2>
    <h3 id="environment">environment</h3>
    <h4 id="meta-world-a-benchmark-and-evaluation-for-multi-task-and-meta-reinforcement-learning">Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</h4>
  </li>
  <li>source: PMLR 2020</li>
  <li>method: None</li>
  <li>environment:
    <ul>
      <li>object manipulation</li>
    </ul>
  </li>
  <li>paper link: <a href="http://proceedings.mlr.press/v100/yu20a/yu20a.pdf">http://proceedings.mlr.press/v100/yu20a/yu20a.pdf</a></li>
  <li>code:  <a href="https://github.com/rlworkgroup/metaworld">https://github.com/rlworkgroup/metaworld</a></li>
  <li>interpretation:
    <ul>
      <li><a href="https://meta-world.github.io/">https://meta-world.github.io/</a></li>
    </ul>
  </li>
</ul>

<h3 id="model-based-meta-rl">model-based meta-RL</h3>
<h4 id="learning-to-reinforcement-learn"><a href="http://localhost:4000/_posts/2021-06-01-Learning-to-reinforcement-learn.md">Learning to reinforcement learn</a></h4>
<ul>
  <li>source: CogSci 2017</li>
  <li>method: deep meta-RL</li>
  <li>environment:
    <ul>
      <li>bandit problem</li>
      <li>Two-step task</li>
      <li>Harlow experiment</li>
      <li>3D navigation (Deepmind Lab)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1611.05763.pdf">https://arxiv.org/pdf/1611.05763.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="rl2-fast-reinforcement-learning-via-slow-reinforcement-learning"><a href="http://localhost:4000/_posts/2021-06-02-RL2.md">RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning</a></h4>
<ul>
  <li>source: ICLR 2017</li>
  <li>method: $RL^2$</li>
  <li>environment:
    <ul>
      <li>multi-armed bandit problem</li>
      <li>tabular MDP</li>
      <li>3D navigation (ViZDoom)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1611.02779.pdf">https://arxiv.org/pdf/1611.02779.pdf</a></li>
  <li>code:</li>
  <li>interpretation:
    <ul>
      <li><a href="https://zhuanlan.zhihu.com/p/32606591">https://zhuanlan.zhihu.com/p/32606591</a></li>
      <li><a href="https://openreview.net/forum?id=HkLXCE9lx">https://openreview.net/forum?id=HkLXCE9lx</a></li>
    </ul>
  </li>
</ul>

<h4 id="prefrontal-cortex-as-a-meta-reinforcement-learning-system">Prefrontal cortex as a meta-reinforcement learning system</h4>
<ul>
  <li>source: Nature Neuroscience 2018</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://www.nature.com/articles/s41593-018-0147-8">https://www.nature.com/articles/s41593-018-0147-8</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="a-simple-neural-attentive-meta-learner">A Simple Neural Attentive Meta-Learner</h4>
<ul>
  <li>source: ICLR 2018</li>
  <li>method: SNAIL (simple neural attentive learner)</li>
  <li>environment:
    <ul>
      <li>navigation</li>
      <li>robotic locomotion</li>
    </ul>
  </li>
  <li>paper link: <a href="https://openreview.net/pdf?id=B1DmUzWAW">https://openreview.net/pdf?id=B1DmUzWAW</a></li>
  <li>code: <a href="https://github.com/eambutu/snail-pytorch">https://github.com/eambutu/snail-pytorch</a></li>
  <li>interpretation:
    <ul>
      <li><a href="https://www.carsi.edu.cn/index_zh.htm">https://www.carsi.edu.cn/index_zh.htm</a></li>
    </ul>
  </li>
</ul>

<h4 id="pixelsnail-an-improved-autoregressive-generative-model">PixelSNAIL: An Improved Autoregressive Generative Model</h4>
<ul>
  <li>source: ICML 2018</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1712.09763v1.pdf">https://arxiv.org/pdf/1712.09763v1.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/neocxi/pixelsnail-public">https://github.com/neocxi/pixelsnail-public</a></li>
    </ul>
  </li>
  <li>interpretation:</li>
</ul>

<h4 id="concurrent-meta-reinforcement-learning">Concurrent Meta Reinforcement Learning</h4>
<ul>
  <li>source: arXiv:1903.02710 preprint</li>
  <li>method: CMRL</li>
  <li>environment:
    <ul>
      <li>N-Monty-Hall</li>
      <li>N-Color-Choice</li>
      <li>N-Reacher (Reacher-V2 from gym)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1903.02710v1.pdf">https://arxiv.org/pdf/1903.02710v1.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="reinforcement-learning-fast-and-slow">Reinforcement Learning, Fast and Slow</h4>
<ul>
  <li>source: Trends in Cognitive Sciences 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0">https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="improving-generalization-in-meta-reinforcement-learning-using-learned-objectives">Improving Generalization in Meta Reinforcement Learning using Learned Objectives</h4>
<ul>
  <li>source: ICLR 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1910.04098.pdf">https://arxiv.org/pdf/1910.04098.pdf</a></li>
  <li>code: <a href="https://github.com/louiskirsch/metagenrl">https://github.com/louiskirsch/metagenrl</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="discovering-reinforcement-learning-algorithms">Discovering Reinforcement Learning Algorithms</h4>
<ul>
  <li>source:	arXiv:2007.08794 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2007.08794">https://arxiv.org/pdf/2007.08794</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="model-based-adversarial-meta-reinforcement-learning">Model-based Adversarial Meta-Reinforcement Learning</h4>
<ul>
  <li>source: NeurIPS 2020</li>
  <li>method: AdMRL</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2006.08875v2.pdf">https://arxiv.org/pdf/2006.08875v2.pdf</a></li>
  <li>code: <a href="https://github.com/LinZichuan/AdMRL">https://github.com/LinZichuan/AdMRL</a></li>
  <li>interpretation:</li>
</ul>

<h3 id="optimization-based-meta-rl">optimization-based meta-RL</h3>
<h4 id="model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks"><a href="https://tianyma.github.io/2021/06/03/MAML.html">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></h4>
<ul>
  <li>source: ICML 2017</li>
  <li>method: MAML-RL</li>
  <li>environment:
    <ul>
      <li>2D navigation (rllab)</li>
      <li>locomotion (rllab)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1703.03400.pdf">https://arxiv.org/pdf/1703.03400.pdf</a></li>
  <li>code: <a href="https://github.com/cbfinn/maml_rl">https://github.com/cbfinn/maml_rl</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="on-first-order-meta-learning-algorithms"><a href="https://tianyma.github.io/2021/06/04/Reptile.html">On First-Order Meta-Learning Algorithms</a></h4>
<ul>
  <li>source:	arXiv:1803.02999 2018</li>
  <li>method: Reptile</li>
  <li>environment:
    <ul>
      <li>few-shot image classification
        <ul>
          <li>mini-ImageNet</li>
          <li>Omniglot</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1803.02999.pdf">https://arxiv.org/pdf/1803.02999.pdf</a></li>
  <li>code: <a href="https://github.com/openai/supervised-reptile">https://github.com/openai/supervised-reptile</a></li>
  <li>interpretation:
    <ul>
      <li>https://openai.com/blog/reptile/</li>
      <li>https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#reptile</li>
    </ul>
  </li>
</ul>

<h4 id="meta-reinforcement-learning-of-structured-exploration-strategies">Meta-Reinforcement Learning of Structured Exploration Strategies</h4>
<ul>
  <li>source: NeurIPS 2018</li>
  <li>method: MAESN (model agnostic exploration with structured noise)</li>
  <li>environment:
    <ul>
      <li>robotic locomotion (rllab)</li>
      <li>object manipulation</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1802.07245.pdf">https://arxiv.org/pdf/1802.07245.pdf</a></li>
  <li>code: <a href="https://github.com/russellmendonca/maesn_suite">https://github.com/russellmendonca/maesn_suite</a></li>
  <li>interpretation:
    <ul>
      <li><a href="https://zhuanlan.zhihu.com/p/63072582">https://zhuanlan.zhihu.com/p/63072582</a></li>
    </ul>
  </li>
</ul>

<h4 id="some-considerations-on-learning-to-explore-via-meta-reinforcement-learning">Some Considerations on Learning to Explore via Meta-Reinforcement Learning</h4>
<ul>
  <li>source: ICLR 2018</li>
  <li>method:
    <ul>
      <li>E-MAML(optimization-based)</li>
      <li>E-$RL^2$(model-based)</li>
    </ul>
  </li>
  <li>environment:
    <ul>
      <li><a href="https://github.com/bstadie/krazyworld">Krazy World</a></li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1803.01118v2.pdf">https://arxiv.org/pdf/1803.01118v2.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/geyang/e-maml">https://github.com/geyang/e-maml</a></li>
    </ul>
  </li>
  <li>interpretation:</li>
</ul>

<h4 id="promp-proximal-meta-policy-search">ProMP: Proximal Meta-Policy Search</h4>
<ul>
  <li>source: ICLR 2019</li>
  <li>method: ProMP</li>
  <li>environment:
    <ul>
      <li>locomotion(gym &amp; Mujoco)
        <ul>
          <li>HalfCheetahFwdBack</li>
          <li>AntRandDir</li>
          <li>HopperRandParams</li>
          <li>WalkerFwdBack</li>
          <li>HumanoidRandDir</li>
          <li>WalkerRandParams</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>paper link: <a href="https://openreview.net/pdf?id=SkxXCi0qFX">https://openreview.net/pdf?id=SkxXCi0qFX</a></li>
  <li>code: <a href="https://github.com/jonasrothfuss/promp">https://github.com/jonasrothfuss/promp</a></li>
</ul>

<h4 id="efficient-off-policy-meta-reinforcement-learning-via-probabilistic-context-variables">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</h4>
<ul>
  <li>source: ICML2019</li>
  <li>method: PEARL (probabilistic embeddings for actor-critic RL)</li>
  <li>environment:
    <ul>
      <li>robotic locomotion (MuJoCo, MuJoCo200, MuJoCu133)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1903.08254.pdf">https://arxiv.org/pdf/1903.08254.pdf</a></li>
  <li>code: <a href="https://github.com/katerakelly/oyster">https://github.com/katerakelly/oyster</a></li>
  <li>interpretation:
    <ul>
      <li><a href="https://bair.berkeley.edu/blog/2019/06/10/pearl/">https://bair.berkeley.edu/blog/2019/06/10/pearl/</a></li>
    </ul>
  </li>
</ul>

<h4 id="learning-to-adapt-in-dynamic-real-world-environments-through-meta-reinforcement-learning">Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning</h4>
<ul>
  <li>source: ICLR 2019</li>
  <li>method:
    <ul>
      <li>Model-Based Meta-Reinforcement Learning (train time)</li>
      <li>Online Model Adaptation (test time)</li>
    </ul>
  </li>
  <li>environment:
    <ul>
      <li>Mujoco
        <ul>
          <li>half cheetah:disabled joint, sloped terrain, pier</li>
          <li>Ant: crippled leg</li>
        </ul>
      </li>
      <li>real world robot</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1803.11347v6.pdf">https://arxiv.org/pdf/1803.11347v6.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/iclavera/learning_to_adapt">https://github.com/iclavera/learning_to_adapt</a></li>
    </ul>
  </li>
  <li>interpretation:
    <ul>
      <li><a href="https://sites.google.com/berkeley.edu/metaadaptivecontrol">https://sites.google.com/berkeley.edu/metaadaptivecontrol</a></li>
    </ul>
  </li>
</ul>

<h4 id="meta-q-learning">Meta-Q-Learning</h4>
<ul>
  <li>source: ICLR 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1910.00125.pdf">https://arxiv.org/pdf/1910.00125.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="decoupling-exploration-and-exploitation-for-meta-reinforcement-learning-without-sacrifices">Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</h4>
<ul>
  <li>source:	ICML 2021</li>
  <li>method: Dream</li>
  <li>environment:
    <ul>
      <li><a href="https://github.com/maximecb/gym-miniworld">gym-miniworld</a></li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/2008.02790v2.pdf">https://arxiv.org/pdf/2008.02790v2.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/ezliu/dream">https://github.com/ezliu/dream</a></li>
    </ul>
  </li>
  <li>interpretation:
    <ul>
      <li><a href="https://ezliu.github.io/dream/">https://ezliu.github.io/dream/</a></li>
    </ul>
  </li>
</ul>

<h4 id="meta-learning-via-learned-loss">Meta Learning via Learned Loss</h4>
<ul>
  <li>source: ICPR 2021</li>
  <li>method: ML^3</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1906.05374.pdf">https://arxiv.org/pdf/1906.05374.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h3 id="未分类">未分类</h3>
<h4 id="alchemy-a-structured-task-distribution-for-meta-reinforcement-learning">Alchemy: A structured task distribution for meta-reinforcement learning</h4>
<ul>
  <li>source: arXiv:2102.02926 preprint 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2102.02926v1.pdf">https://arxiv.org/pdf/2102.02926v1.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/deepmind/dm_alchemy">https://github.com/deepmind/dm_alchemy</a></li>
    </ul>
  </li>
  <li>interpretation:
    <ul>
      <li><a href="https://deepmind.com/research/publications/alchemy">https://deepmind.com/research/publications/alchemy</a></li>
    </ul>
  </li>
</ul>

<h4 id="learning-to-learn-how-to-learn-self-adaptive-visual-navigation-using-meta-learning">Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning</h4>
<ul>
  <li>source: CVPR 2019</li>
  <li>method: savn</li>
  <li>environment:
    <ul>
      <li>3D navigation (ai2thor)</li>
    </ul>
  </li>
  <li>paper link: <a href="https://arxiv.org/pdf/1812.00971v2.pdf">https://arxiv.org/pdf/1812.00971v2.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/allenai/savn">https://github.com/allenai/savn</a></li>
    </ul>
  </li>
  <li>interpretation:</li>
</ul>

<h4 id="learning-robust-state-abstractions-for-hidden-parameter-block-mdps">Learning Robust State Abstractions for Hidden-Parameter Block MDPs</h4>
<ul>
  <li>source: ICLR2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2007.07206v4.pdf">https://arxiv.org/pdf/2007.07206v4.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="meta-reinforcement-learning-as-task-inference">Meta reinforcement learning as task inference</h4>
<ul>
  <li>source:	arXiv:1905.06424 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1905.06424v2.pdf">https://arxiv.org/pdf/1905.06424v2.pdf</a></li>
  <li>code:</li>
  <li>interpretation:</li>
</ul>

<h4 id="meld-meta-reinforcement-learning-from-images-via-latent-state-models">MELD: Meta-Reinforcement Learning from Images via Latent State Models</h4>
<ul>
  <li>source: CoRL 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2010.13957v2.pdf">https://arxiv.org/pdf/2010.13957v2.pdf</a></li>
  <li>code:
    <ul>
      <li><a href="https://github.com/tonyzhaozh/meld">https://github.com/tonyzhaozh/meld</a></li>
    </ul>
  </li>
  <li>interpretation:
    <ul>
      <li><a href="https://sites.google.com/view/meld-lsm">https://sites.google.com/view/meld-lsm</a></li>
    </ul>
  </li>
</ul>

<h4 id="meta-reinforcement-learning-with-task-embedding-and-shared-policy">Meta Reinforcement Learning with Task Embedding and Shared Policy</h4>
<ul>
  <li>source: /IJCAI 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1905.06527v3.pdf">https://arxiv.org/pdf/1905.06527v3.pdf</a></li>
  <li>code: <a href="https://github.com/llan-ml/tesp">https://github.com/llan-ml/tesp</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="fast-adaptive-task-offloading-in-edge-computing-based-on-meta-reinforcement-learning">Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning</h4>
<ul>
  <li>source: ITPDS 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2008.02033v5.pdf">https://arxiv.org/pdf/2008.02033v5.pdf</a></li>
  <li>code: <a href="https://github.com/linkpark/metarl-offloading">https://github.com/linkpark/metarl-offloading</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="learning-associative-inference-using-fast-weight-memory">Learning Associative Inference Using Fast Weight Memory</h4>
<ul>
  <li>source: ICLR 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2011.07831v2.pdf">https://arxiv.org/pdf/2011.07831v2.pdf</a></li>
  <li>code: <a href="https://github.com/ischlag/Fast-Weight-Memory-public">https://github.com/ischlag/Fast-Weight-Memory-public</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="few-shot-complex-knowledge-base-question-answering-via-meta-reinforcement-learning">Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning</h4>
<ul>
  <li>source: EMNLP 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2010.15877v1.pdf">https://arxiv.org/pdf/2010.15877v1.pdf</a></li>
  <li>code: <a href="https://github.com/DevinJake/MRL-CQA">https://github.com/DevinJake/MRL-CQA</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="meta-reinforcement-learning-with-autonomous-inference-of-subtask-dependencies">Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies</h4>
<ul>
  <li>source: ICLR 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2001.00248v2.pdf">https://arxiv.org/pdf/2001.00248v2.pdf</a></li>
  <li>code: <a href="https://github.com/srsohn/msgi">https://github.com/srsohn/msgi</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning">Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning</h4>
<ul>
  <li>source: NeurIPS 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf">http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf</a></li>
  <li>code: <a href="https://github.com/oxwhirl/loaded-dice">https://github.com/oxwhirl/loaded-dice</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="causal-reasoning-from-meta-reinforcement-learning">Causal Reasoning from Meta-reinforcement Learning</h4>
<ul>
  <li>source: ICLR 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1901.08162v1.pdf">https://arxiv.org/pdf/1901.08162v1.pdf</a></li>
  <li>code: <a href="https://github.com/kantneel/causal-metarl">https://github.com/kantneel/causal-metarl</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="introducing-neuromodulation-in-deep-neural-networks-to-learn-adaptive-behaviours">Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours</h4>
<ul>
  <li>source:	arXiv:1812.09113 preprint 2019</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/1812.09113v3.pdf">https://arxiv.org/pdf/1812.09113v3.pdf</a></li>
  <li>code: <a href="https://github.com/nvecoven/nmd_net">https://github.com/nvecoven/nmd_net</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="policy-gradient-rl-algorithms-as-directed-acyclic-graphs">Policy Gradient RL Algorithms as Directed Acyclic Graphs</h4>
<ul>
  <li>source: 	arXiv:2012.07763 preprint 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2012.07763v2.pdf">https://arxiv.org/pdf/2012.07763v2.pdf</a></li>
  <li>code: <a href="https://github.com/jjgarau/DAGPolicyGradient">https://github.com/jjgarau/DAGPolicyGradient</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="evolving-inborn-knowledge-for-fast-adaptation-in-dynamic-pomdp-problems">Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems</h4>
<ul>
  <li>source: GECCO 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2004.12846v2.pdf">https://arxiv.org/pdf/2004.12846v2.pdf</a></li>
  <li>code: <a href="https://github.com/dlpbc/penn-a">https://github.com/dlpbc/penn-a</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="model-based-meta-reinforcement-learning-for-flight-with-suspended-payloads">Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads</h4>
<ul>
  <li>source: RA-L 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2004.11345v2.pdf">https://arxiv.org/pdf/2004.11345v2.pdf</a></li>
  <li>code: <a href="https://github.com/suneelbelkhale/model-based-meta-rl-for-flight">https://github.com/suneelbelkhale/model-based-meta-rl-for-flight</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="hierarchical-meta-reinforcement-learning-for-multi-task-environments">Hierarchical Meta Reinforcement Learning for Multi-Task Environments</h4>
<ul>
  <li>source: ICLR 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://openreview.net/pdf?id=u9ax42K7ND">https://openreview.net/pdf?id=u9ax42K7ND</a></li>
  <li>code: <a href="https://github.com/MeSH-ICLR/MEtaSoftHierarchy">https://github.com/MeSH-ICLR/MEtaSoftHierarchy</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="modeling-and-optimization-trade-off-in-meta-learning">Modeling and Optimization Trade-off in Meta-learning</h4>
<ul>
  <li>source: NeurIPS 2020</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2010.12916v2.pdf">https://arxiv.org/pdf/2010.12916v2.pdf</a></li>
  <li>code: <a href="https://github.com/intel-isl/MetaLearningTradeoffs">https://github.com/intel-isl/MetaLearningTradeoffs</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="meta-learning-of-structured-task-distributions-in-humans-and-machines">Meta-Learning of Structured Task Distributions in Humans and Machines</h4>
<ul>
  <li>source: ICLR 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2010.02317v3.pdf">https://arxiv.org/pdf/2010.02317v3.pdf</a></li>
  <li>code: <a href="https://github.com/sreejank/Compositional_MetaRL">https://github.com/sreejank/Compositional_MetaRL</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="offline-meta-learning-of-exploration">Offline Meta Learning of Exploration</h4>
<ul>
  <li>source:	arXiv:2008.02598 preprint 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2008.02598v3.pdf">https://arxiv.org/pdf/2008.02598v3.pdf</a></li>
  <li>code: <a href="https://github.com/Rondorf/BOReL">https://github.com/Rondorf/BOReL</a></li>
  <li>interpretation:</li>
</ul>

<h4 id="meta-reinforcement-learning-for-reliable-communication-in-thzvlc-wireless-vr-networks">Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks</h4>
<ul>
  <li>source: ICC 2021</li>
  <li>method:</li>
  <li>environment:</li>
  <li>paper link: <a href="https://arxiv.org/pdf/2102.12277v1.pdf">https://arxiv.org/pdf/2102.12277v1.pdf</a></li>
  <li>code: <a href="https://github.com/wyy0206/THzVR">https://github.com/wyy0206/THzVR</a></li>
  <li>interpretation:</li>
</ul>

<!--
#### 
- source: 
- method: 
- environment:
- paper link: 
- code:  
- interpretation: 

-->
:ET