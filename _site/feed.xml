<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2021-06-03T23:29:53+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">tianyma’s blog</title><subtitle>do valuable research, be valuable person.
</subtitle><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><entry><title type="html">Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title><link href="http://localhost:4000/2021/06/03/MAML.html" rel="alternate" type="text/html" title="Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" /><published>2021-06-03T00:00:00+08:00</published><updated>2021-06-03T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/03/MAML</id><content type="html" xml:base="http://localhost:4000/2021/06/03/MAML.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 &lt;br /&gt;
MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。  &lt;br /&gt;
由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。 实现过程 以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。 由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。 测试任务 在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。 创新点 MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。 算法评价 MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。 返回文章列表</summary></entry><entry><title type="html">Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning</title><link href="http://localhost:4000/2021/06/02/RL2.html" rel="alternate" type="text/html" title="Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning" /><published>2021-06-02T00:00:00+08:00</published><updated>2021-06-02T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/02/RL2</id><content type="html" xml:base="http://localhost:4000/2021/06/02/RL2.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;episode:&lt;/strong&gt; agent和一个MDP的一次开始到结束的交互序列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;trial:&lt;/strong&gt; agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。&lt;/p&gt;

&lt;p&gt;agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了&lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;TRPO算法&lt;/a&gt;。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-armed Bandit&lt;/li&gt;
  &lt;li&gt;Tabular MDP&lt;/li&gt;
  &lt;li&gt;3D maze navigation(ViZDoom environment)
    &lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
    &lt;p&gt;这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。&lt;/p&gt;
    &lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
    &lt;p&gt;这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。 实现过程 定义 episode: agent和一个MDP的一次开始到结束的交互序列。 trial: agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。 agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了TRPO算法。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。 测试任务 Multi-armed Bandit Tabular MDP 3D maze navigation(ViZDoom environment) 创新点 这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。 算法评价 这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Learning to reinforcement learn</title><link href="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html" rel="alternate" type="text/html" title="Paper reading: Learning to reinforcement learn" /><published>2021-06-01T00:00:00+08:00</published><updated>2021-06-01T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn</id><content type="html" xml:base="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。&lt;/p&gt;

&lt;p&gt;训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的&lt;a href=&quot;https://scienceofbehaviorchange.org/measures/two-stage-task/&quot;&gt;two step task&lt;/a&gt;和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。 实现过程 本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。 训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。 测试任务 本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的two step task和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。 创新点 本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。 算法评价 返回文章列表</summary></entry><entry><title type="html">Paper review: meta-reinforcement-learning</title><link href="http://localhost:4000/2021/05/29/meta-reinforcement-learning.html" rel="alternate" type="text/html" title="Paper review: meta-reinforcement-learning" /><published>2021-05-29T00:00:00+08:00</published><updated>2021-05-29T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/29/meta-reinforcement-learning</id><content type="html" xml:base="http://localhost:4000/2021/05/29/meta-reinforcement-learning.html">&lt;h2 id=&quot;challenges-of-meta-rl&quot;&gt;Challenges of meta-RL&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;design a set of tasks that are interrelated&lt;/li&gt;
  &lt;li&gt;find the inter-representation&lt;/li&gt;
  &lt;li&gt;fast adaptation to new tasks
    &lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;
    &lt;h3 id=&quot;environment&quot;&gt;environment&lt;/h3&gt;
    &lt;h4 id=&quot;meta-world-a-benchmark-and-evaluation-for-multi-task-and-meta-reinforcement-learning&quot;&gt;Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;source: PMLR 2020&lt;/li&gt;
  &lt;li&gt;method: None&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;object manipulation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;http://proceedings.mlr.press/v100/yu20a/yu20a.pdf&quot;&gt;http://proceedings.mlr.press/v100/yu20a/yu20a.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:  &lt;a href=&quot;https://github.com/rlworkgroup/metaworld&quot;&gt;https://github.com/rlworkgroup/metaworld&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://meta-world.github.io/&quot;&gt;https://meta-world.github.io/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-based-meta-rl&quot;&gt;model-based meta-RL&lt;/h3&gt;
&lt;h4 id=&quot;learning-to-reinforcement-learn&quot;&gt;&lt;a href=&quot;https://tianyma.github.io/2021/06/01/Learning-to-reinforcement-learn.html&quot;&gt;Learning to reinforcement learn&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: CogSci 2017&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;bandit problem&lt;/li&gt;
      &lt;li&gt;Two-step task&lt;/li&gt;
      &lt;li&gt;Harlow experiment&lt;/li&gt;
      &lt;li&gt;3D navigation (Deepmind Lab)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1611.05763.pdf&quot;&gt;https://arxiv.org/pdf/1611.05763.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;rl2-fast-reinforcement-learning-via-slow-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://tianyma.github.io/2021/06/02/RL2.html&quot;&gt;RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2017&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;multi-armed bandit problem&lt;/li&gt;
      &lt;li&gt;tabular MDP&lt;/li&gt;
      &lt;li&gt;3D navigation (ViZDoom)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1611.02779.pdf&quot;&gt;https://arxiv.org/pdf/1611.02779.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32606591&quot;&gt;https://zhuanlan.zhihu.com/p/32606591&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=HkLXCE9lx&quot;&gt;https://openreview.net/forum?id=HkLXCE9lx&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;prefrontal-cortex-as-a-meta-reinforcement-learning-system&quot;&gt;Prefrontal cortex as a meta-reinforcement learning system&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: Nature Neuroscience 2018&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://www.nature.com/articles/s41593-018-0147-8&quot;&gt;https://www.nature.com/articles/s41593-018-0147-8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;a-simple-neural-attentive-meta-learner&quot;&gt;A Simple Neural Attentive Meta-Learner&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2018&lt;/li&gt;
  &lt;li&gt;method: SNAIL (simple neural attentive learner)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;navigation&lt;/li&gt;
      &lt;li&gt;robotic locomotion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=B1DmUzWAW&quot;&gt;https://openreview.net/pdf?id=B1DmUzWAW&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/eambutu/snail-pytorch&quot;&gt;https://github.com/eambutu/snail-pytorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.carsi.edu.cn/index_zh.htm&quot;&gt;https://www.carsi.edu.cn/index_zh.htm&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pixelsnail-an-improved-autoregressive-generative-model&quot;&gt;PixelSNAIL: An Improved Autoregressive Generative Model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML 2018&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1712.09763v1.pdf&quot;&gt;https://arxiv.org/pdf/1712.09763v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/neocxi/pixelsnail-public&quot;&gt;https://github.com/neocxi/pixelsnail-public&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;concurrent-meta-reinforcement-learning&quot;&gt;Concurrent Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: arXiv:1903.02710 preprint&lt;/li&gt;
  &lt;li&gt;method: CMRL&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;N-Monty-Hall&lt;/li&gt;
      &lt;li&gt;N-Color-Choice&lt;/li&gt;
      &lt;li&gt;N-Reacher (Reacher-V2 from gym)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1903.02710v1.pdf&quot;&gt;https://arxiv.org/pdf/1903.02710v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reinforcement-learning-fast-and-slow&quot;&gt;Reinforcement Learning, Fast and Slow&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: Trends in Cognitive Sciences 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0&quot;&gt;https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;improving-generalization-in-meta-reinforcement-learning-using-learned-objectives&quot;&gt;Improving Generalization in Meta Reinforcement Learning using Learned Objectives&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1910.04098.pdf&quot;&gt;https://arxiv.org/pdf/1910.04098.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/louiskirsch/metagenrl&quot;&gt;https://github.com/louiskirsch/metagenrl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;discovering-reinforcement-learning-algorithms&quot;&gt;Discovering Reinforcement Learning Algorithms&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:2007.08794 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2007.08794&quot;&gt;https://arxiv.org/pdf/2007.08794&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-based-adversarial-meta-reinforcement-learning&quot;&gt;Model-based Adversarial Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2020&lt;/li&gt;
  &lt;li&gt;method: AdMRL&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2006.08875v2.pdf&quot;&gt;https://arxiv.org/pdf/2006.08875v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/LinZichuan/AdMRL&quot;&gt;https://github.com/LinZichuan/AdMRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-based-meta-rl&quot;&gt;optimization-based meta-RL&lt;/h3&gt;
&lt;h4 id=&quot;model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks&quot;&gt;&lt;a href=&quot;https://tianyma.github.io/2021/06/03/MAML.html&quot;&gt;Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML 2017&lt;/li&gt;
  &lt;li&gt;method: MAML-RL&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;2D navigation (rllab)&lt;/li&gt;
      &lt;li&gt;locomotion (rllab)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1703.03400.pdf&quot;&gt;https://arxiv.org/pdf/1703.03400.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/cbfinn/maml_rl&quot;&gt;https://github.com/cbfinn/maml_rl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;on-first-order-meta-learning-algorithms&quot;&gt;On First-Order Meta-Learning Algorithms&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1803.02999 2018&lt;/li&gt;
  &lt;li&gt;method: Reptile&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.02999.pdf&quot;&gt;https://arxiv.org/pdf/1803.02999.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/openai/supervised-reptile&quot;&gt;https://github.com/openai/supervised-reptile&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-of-structured-exploration-strategies&quot;&gt;Meta-Reinforcement Learning of Structured Exploration Strategies&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2018&lt;/li&gt;
  &lt;li&gt;method: MAESN (model agnostic exploration with structured noise)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;robotic locomotion (rllab)&lt;/li&gt;
      &lt;li&gt;object manipulation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1802.07245.pdf&quot;&gt;https://arxiv.org/pdf/1802.07245.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/russellmendonca/maesn_suite&quot;&gt;https://github.com/russellmendonca/maesn_suite&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/63072582&quot;&gt;https://zhuanlan.zhihu.com/p/63072582&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;some-considerations-on-learning-to-explore-via-meta-reinforcement-learning&quot;&gt;Some Considerations on Learning to Explore via Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2018&lt;/li&gt;
  &lt;li&gt;method:
    &lt;ul&gt;
      &lt;li&gt;E-MAML(optimization-based)&lt;/li&gt;
      &lt;li&gt;E-$RL^2$(model-based)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/bstadie/krazyworld&quot;&gt;Krazy World&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.01118v2.pdf&quot;&gt;https://arxiv.org/pdf/1803.01118v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/geyang/e-maml&quot;&gt;https://github.com/geyang/e-maml&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;promp-proximal-meta-policy-search&quot;&gt;ProMP: Proximal Meta-Policy Search&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method: ProMP&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;locomotion(gym &amp;amp; Mujoco)
        &lt;ul&gt;
          &lt;li&gt;HalfCheetahFwdBack&lt;/li&gt;
          &lt;li&gt;AntRandDir&lt;/li&gt;
          &lt;li&gt;HopperRandParams&lt;/li&gt;
          &lt;li&gt;WalkerFwdBack&lt;/li&gt;
          &lt;li&gt;HumanoidRandDir&lt;/li&gt;
          &lt;li&gt;WalkerRandParams&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=SkxXCi0qFX&quot;&gt;https://openreview.net/pdf?id=SkxXCi0qFX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/jonasrothfuss/promp&quot;&gt;https://github.com/jonasrothfuss/promp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;efficient-off-policy-meta-reinforcement-learning-via-probabilistic-context-variables&quot;&gt;Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICML2019&lt;/li&gt;
  &lt;li&gt;method: PEARL (probabilistic embeddings for actor-critic RL)&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;robotic locomotion (MuJoCo, MuJoCo200, MuJoCu133)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1903.08254.pdf&quot;&gt;https://arxiv.org/pdf/1903.08254.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/katerakelly/oyster&quot;&gt;https://github.com/katerakelly/oyster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://bair.berkeley.edu/blog/2019/06/10/pearl/&quot;&gt;https://bair.berkeley.edu/blog/2019/06/10/pearl/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-to-adapt-in-dynamic-real-world-environments-through-meta-reinforcement-learning&quot;&gt;Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method:
    &lt;ul&gt;
      &lt;li&gt;Model-Based Meta-Reinforcement Learning (train time)&lt;/li&gt;
      &lt;li&gt;Online Model Adaptation (test time)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;Mujoco
        &lt;ul&gt;
          &lt;li&gt;half cheetah:disabled joint, sloped terrain, pier&lt;/li&gt;
          &lt;li&gt;Ant: crippled leg&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;real world robot&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1803.11347v6.pdf&quot;&gt;https://arxiv.org/pdf/1803.11347v6.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/iclavera/learning_to_adapt&quot;&gt;https://github.com/iclavera/learning_to_adapt&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://sites.google.com/berkeley.edu/metaadaptivecontrol&quot;&gt;https://sites.google.com/berkeley.edu/metaadaptivecontrol&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-q-learning&quot;&gt;Meta-Q-Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1910.00125.pdf&quot;&gt;https://arxiv.org/pdf/1910.00125.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;decoupling-exploration-and-exploitation-for-meta-reinforcement-learning-without-sacrifices&quot;&gt;Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	ICML 2021&lt;/li&gt;
  &lt;li&gt;method: Dream&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/maximecb/gym-miniworld&quot;&gt;gym-miniworld&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02790v2.pdf&quot;&gt;https://arxiv.org/pdf/2008.02790v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/ezliu/dream&quot;&gt;https://github.com/ezliu/dream&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ezliu.github.io/dream/&quot;&gt;https://ezliu.github.io/dream/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-learning-via-learned-loss&quot;&gt;Meta Learning via Learned Loss&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICPR 2021&lt;/li&gt;
  &lt;li&gt;method: ML^3&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1906.05374.pdf&quot;&gt;https://arxiv.org/pdf/1906.05374.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;未分类&quot;&gt;未分类&lt;/h3&gt;
&lt;h4 id=&quot;alchemy-a-structured-task-distribution-for-meta-reinforcement-learning&quot;&gt;Alchemy: A structured task distribution for meta-reinforcement learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: arXiv:2102.02926 preprint 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2102.02926v1.pdf&quot;&gt;https://arxiv.org/pdf/2102.02926v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/deepmind/dm_alchemy&quot;&gt;https://github.com/deepmind/dm_alchemy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://deepmind.com/research/publications/alchemy&quot;&gt;https://deepmind.com/research/publications/alchemy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-to-learn-how-to-learn-self-adaptive-visual-navigation-using-meta-learning&quot;&gt;Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: CVPR 2019&lt;/li&gt;
  &lt;li&gt;method: savn&lt;/li&gt;
  &lt;li&gt;environment:
    &lt;ul&gt;
      &lt;li&gt;3D navigation (ai2thor)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1812.00971v2.pdf&quot;&gt;https://arxiv.org/pdf/1812.00971v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/allenai/savn&quot;&gt;https://github.com/allenai/savn&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-robust-state-abstractions-for-hidden-parameter-block-mdps&quot;&gt;Learning Robust State Abstractions for Hidden-Parameter Block MDPs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2007.07206v4.pdf&quot;&gt;https://arxiv.org/pdf/2007.07206v4.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-as-task-inference&quot;&gt;Meta reinforcement learning as task inference&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1905.06424 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1905.06424v2.pdf&quot;&gt;https://arxiv.org/pdf/1905.06424v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meld-meta-reinforcement-learning-from-images-via-latent-state-models&quot;&gt;MELD: Meta-Reinforcement Learning from Images via Latent State Models&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: CoRL 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.13957v2.pdf&quot;&gt;https://arxiv.org/pdf/2010.13957v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/tonyzhaozh/meld&quot;&gt;https://github.com/tonyzhaozh/meld&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;interpretation:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://sites.google.com/view/meld-lsm&quot;&gt;https://sites.google.com/view/meld-lsm&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-with-task-embedding-and-shared-policy&quot;&gt;Meta Reinforcement Learning with Task Embedding and Shared Policy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: /IJCAI 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1905.06527v3.pdf&quot;&gt;https://arxiv.org/pdf/1905.06527v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/llan-ml/tesp&quot;&gt;https://github.com/llan-ml/tesp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;fast-adaptive-task-offloading-in-edge-computing-based-on-meta-reinforcement-learning&quot;&gt;Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ITPDS 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02033v5.pdf&quot;&gt;https://arxiv.org/pdf/2008.02033v5.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/linkpark/metarl-offloading&quot;&gt;https://github.com/linkpark/metarl-offloading&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-associative-inference-using-fast-weight-memory&quot;&gt;Learning Associative Inference Using Fast Weight Memory&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2011.07831v2.pdf&quot;&gt;https://arxiv.org/pdf/2011.07831v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/ischlag/Fast-Weight-Memory-public&quot;&gt;https://github.com/ischlag/Fast-Weight-Memory-public&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;few-shot-complex-knowledge-base-question-answering-via-meta-reinforcement-learning&quot;&gt;Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: EMNLP 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.15877v1.pdf&quot;&gt;https://arxiv.org/pdf/2010.15877v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/DevinJake/MRL-CQA&quot;&gt;https://github.com/DevinJake/MRL-CQA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-with-autonomous-inference-of-subtask-dependencies&quot;&gt;Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2001.00248v2.pdf&quot;&gt;https://arxiv.org/pdf/2001.00248v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/srsohn/msgi&quot;&gt;https://github.com/srsohn/msgi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning&quot;&gt;Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf&quot;&gt;http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/oxwhirl/loaded-dice&quot;&gt;https://github.com/oxwhirl/loaded-dice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;causal-reasoning-from-meta-reinforcement-learning&quot;&gt;Causal Reasoning from Meta-reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1901.08162v1.pdf&quot;&gt;https://arxiv.org/pdf/1901.08162v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/kantneel/causal-metarl&quot;&gt;https://github.com/kantneel/causal-metarl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;introducing-neuromodulation-in-deep-neural-networks-to-learn-adaptive-behaviours&quot;&gt;Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:1812.09113 preprint 2019&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/1812.09113v3.pdf&quot;&gt;https://arxiv.org/pdf/1812.09113v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/nvecoven/nmd_net&quot;&gt;https://github.com/nvecoven/nmd_net&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;policy-gradient-rl-algorithms-as-directed-acyclic-graphs&quot;&gt;Policy Gradient RL Algorithms as Directed Acyclic Graphs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: 	arXiv:2012.07763 preprint 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2012.07763v2.pdf&quot;&gt;https://arxiv.org/pdf/2012.07763v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/jjgarau/DAGPolicyGradient&quot;&gt;https://github.com/jjgarau/DAGPolicyGradient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evolving-inborn-knowledge-for-fast-adaptation-in-dynamic-pomdp-problems&quot;&gt;Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: GECCO 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2004.12846v2.pdf&quot;&gt;https://arxiv.org/pdf/2004.12846v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/dlpbc/penn-a&quot;&gt;https://github.com/dlpbc/penn-a&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-based-meta-reinforcement-learning-for-flight-with-suspended-payloads&quot;&gt;Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: RA-L 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2004.11345v2.pdf&quot;&gt;https://arxiv.org/pdf/2004.11345v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/suneelbelkhale/model-based-meta-rl-for-flight&quot;&gt;https://github.com/suneelbelkhale/model-based-meta-rl-for-flight&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hierarchical-meta-reinforcement-learning-for-multi-task-environments&quot;&gt;Hierarchical Meta Reinforcement Learning for Multi-Task Environments&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://openreview.net/pdf?id=u9ax42K7ND&quot;&gt;https://openreview.net/pdf?id=u9ax42K7ND&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/MeSH-ICLR/MEtaSoftHierarchy&quot;&gt;https://github.com/MeSH-ICLR/MEtaSoftHierarchy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;modeling-and-optimization-trade-off-in-meta-learning&quot;&gt;Modeling and Optimization Trade-off in Meta-learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: NeurIPS 2020&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.12916v2.pdf&quot;&gt;https://arxiv.org/pdf/2010.12916v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/intel-isl/MetaLearningTradeoffs&quot;&gt;https://github.com/intel-isl/MetaLearningTradeoffs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-learning-of-structured-task-distributions-in-humans-and-machines&quot;&gt;Meta-Learning of Structured Task Distributions in Humans and Machines&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICLR 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2010.02317v3.pdf&quot;&gt;https://arxiv.org/pdf/2010.02317v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/sreejank/Compositional_MetaRL&quot;&gt;https://github.com/sreejank/Compositional_MetaRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;offline-meta-learning-of-exploration&quot;&gt;Offline Meta Learning of Exploration&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source:	arXiv:2008.02598 preprint 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2008.02598v3.pdf&quot;&gt;https://arxiv.org/pdf/2008.02598v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/Rondorf/BOReL&quot;&gt;https://github.com/Rondorf/BOReL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;meta-reinforcement-learning-for-reliable-communication-in-thzvlc-wireless-vr-networks&quot;&gt;Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;source: ICC 2021&lt;/li&gt;
  &lt;li&gt;method:&lt;/li&gt;
  &lt;li&gt;environment:&lt;/li&gt;
  &lt;li&gt;paper link: &lt;a href=&quot;https://arxiv.org/pdf/2102.12277v1.pdf&quot;&gt;https://arxiv.org/pdf/2102.12277v1.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code: &lt;a href=&quot;https://github.com/wyy0206/THzVR&quot;&gt;https://github.com/wyy0206/THzVR&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;interpretation:&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
#### 
- source: 
- method: 
- environment:
- paper link: 
- code:  
- interpretation: 

--&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">Challenges of meta-RL design a set of tasks that are interrelated find the inter-representation fast adaptation to new tasks Papers environment Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning source: PMLR 2020 method: None environment: object manipulation paper link: http://proceedings.mlr.press/v100/yu20a/yu20a.pdf code: https://github.com/rlworkgroup/metaworld interpretation: https://meta-world.github.io/ model-based meta-RL Learning to reinforcement learn source: CogSci 2017 method: environment: bandit problem Two-step task Harlow experiment 3D navigation (Deepmind Lab) paper link: https://arxiv.org/pdf/1611.05763.pdf code: interpretation: RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning source: ICLR 2017 method: environment: multi-armed bandit problem tabular MDP 3D navigation (ViZDoom) paper link: https://arxiv.org/pdf/1611.02779.pdf code: interpretation: https://zhuanlan.zhihu.com/p/32606591 https://openreview.net/forum?id=HkLXCE9lx Prefrontal cortex as a meta-reinforcement learning system source: Nature Neuroscience 2018 method: environment: paper link: https://www.nature.com/articles/s41593-018-0147-8 code: interpretation: A Simple Neural Attentive Meta-Learner source: ICLR 2018 method: SNAIL (simple neural attentive learner) environment: navigation robotic locomotion paper link: https://openreview.net/pdf?id=B1DmUzWAW code: https://github.com/eambutu/snail-pytorch interpretation: https://www.carsi.edu.cn/index_zh.htm PixelSNAIL: An Improved Autoregressive Generative Model source: ICML 2018 method: environment: paper link: https://arxiv.org/pdf/1712.09763v1.pdf code: https://github.com/neocxi/pixelsnail-public interpretation: Concurrent Meta Reinforcement Learning source: arXiv:1903.02710 preprint method: CMRL environment: N-Monty-Hall N-Color-Choice N-Reacher (Reacher-V2 from gym) paper link: https://arxiv.org/pdf/1903.02710v1.pdf code: interpretation: Reinforcement Learning, Fast and Slow source: Trends in Cognitive Sciences 2019 method: environment: paper link: https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66131930061-0 code: interpretation: Improving Generalization in Meta Reinforcement Learning using Learned Objectives source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/1910.04098.pdf code: https://github.com/louiskirsch/metagenrl interpretation: Discovering Reinforcement Learning Algorithms source: arXiv:2007.08794 2020 method: environment: paper link: https://arxiv.org/pdf/2007.08794 code: interpretation: Model-based Adversarial Meta-Reinforcement Learning source: NeurIPS 2020 method: AdMRL environment: paper link: https://arxiv.org/pdf/2006.08875v2.pdf code: https://github.com/LinZichuan/AdMRL interpretation: optimization-based meta-RL Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks source: ICML 2017 method: MAML-RL environment: 2D navigation (rllab) locomotion (rllab) paper link: https://arxiv.org/pdf/1703.03400.pdf code: https://github.com/cbfinn/maml_rl interpretation: On First-Order Meta-Learning Algorithms source: arXiv:1803.02999 2018 method: Reptile environment: paper link: https://arxiv.org/pdf/1803.02999.pdf code: https://github.com/openai/supervised-reptile interpretation: Meta-Reinforcement Learning of Structured Exploration Strategies source: NeurIPS 2018 method: MAESN (model agnostic exploration with structured noise) environment: robotic locomotion (rllab) object manipulation paper link: https://arxiv.org/pdf/1802.07245.pdf code: https://github.com/russellmendonca/maesn_suite interpretation: https://zhuanlan.zhihu.com/p/63072582 Some Considerations on Learning to Explore via Meta-Reinforcement Learning source: ICLR 2018 method: E-MAML(optimization-based) E-$RL^2$(model-based) environment: Krazy World paper link: https://arxiv.org/pdf/1803.01118v2.pdf code: https://github.com/geyang/e-maml interpretation: ProMP: Proximal Meta-Policy Search source: ICLR 2019 method: ProMP environment: locomotion(gym &amp;amp; Mujoco) HalfCheetahFwdBack AntRandDir HopperRandParams WalkerFwdBack HumanoidRandDir WalkerRandParams paper link: https://openreview.net/pdf?id=SkxXCi0qFX code: https://github.com/jonasrothfuss/promp Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables source: ICML2019 method: PEARL (probabilistic embeddings for actor-critic RL) environment: robotic locomotion (MuJoCo, MuJoCo200, MuJoCu133) paper link: https://arxiv.org/pdf/1903.08254.pdf code: https://github.com/katerakelly/oyster interpretation: https://bair.berkeley.edu/blog/2019/06/10/pearl/ Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning source: ICLR 2019 method: Model-Based Meta-Reinforcement Learning (train time) Online Model Adaptation (test time) environment: Mujoco half cheetah:disabled joint, sloped terrain, pier Ant: crippled leg real world robot paper link: https://arxiv.org/pdf/1803.11347v6.pdf code: https://github.com/iclavera/learning_to_adapt interpretation: https://sites.google.com/berkeley.edu/metaadaptivecontrol Meta-Q-Learning source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/1910.00125.pdf code: interpretation: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices source: ICML 2021 method: Dream environment: gym-miniworld paper link: https://arxiv.org/pdf/2008.02790v2.pdf code: https://github.com/ezliu/dream interpretation: https://ezliu.github.io/dream/ Meta Learning via Learned Loss source: ICPR 2021 method: ML^3 environment: paper link: https://arxiv.org/pdf/1906.05374.pdf code: interpretation: 未分类 Alchemy: A structured task distribution for meta-reinforcement learning source: arXiv:2102.02926 preprint 2021 method: environment: paper link: https://arxiv.org/pdf/2102.02926v1.pdf code: https://github.com/deepmind/dm_alchemy interpretation: https://deepmind.com/research/publications/alchemy Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning source: CVPR 2019 method: savn environment: 3D navigation (ai2thor) paper link: https://arxiv.org/pdf/1812.00971v2.pdf code: https://github.com/allenai/savn interpretation: Learning Robust State Abstractions for Hidden-Parameter Block MDPs source: ICLR2021 method: environment: paper link: https://arxiv.org/pdf/2007.07206v4.pdf code: interpretation: Meta reinforcement learning as task inference source: arXiv:1905.06424 2019 method: environment: paper link: https://arxiv.org/pdf/1905.06424v2.pdf code: interpretation: MELD: Meta-Reinforcement Learning from Images via Latent State Models source: CoRL 2020 method: environment: paper link: https://arxiv.org/pdf/2010.13957v2.pdf code: https://github.com/tonyzhaozh/meld interpretation: https://sites.google.com/view/meld-lsm Meta Reinforcement Learning with Task Embedding and Shared Policy source: /IJCAI 2019 method: environment: paper link: https://arxiv.org/pdf/1905.06527v3.pdf code: https://github.com/llan-ml/tesp interpretation: Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning source: ITPDS 2020 method: environment: paper link: https://arxiv.org/pdf/2008.02033v5.pdf code: https://github.com/linkpark/metarl-offloading interpretation: Learning Associative Inference Using Fast Weight Memory source: ICLR 2021 method: environment: paper link: https://arxiv.org/pdf/2011.07831v2.pdf code: https://github.com/ischlag/Fast-Weight-Memory-public interpretation: Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning source: EMNLP 2020 method: environment: paper link: https://arxiv.org/pdf/2010.15877v1.pdf code: https://github.com/DevinJake/MRL-CQA interpretation: Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies source: ICLR 2020 method: environment: paper link: https://arxiv.org/pdf/2001.00248v2.pdf code: https://github.com/srsohn/msgi interpretation: Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning source: NeurIPS 2019 method: environment: paper link: http://papers.nips.cc/paper/9026-loaded-dice-trading-off-bias-and-variance-in-any-order-score-function-gradient-estimators-for-reinforcement-learning.pdf code: https://github.com/oxwhirl/loaded-dice interpretation: Causal Reasoning from Meta-reinforcement Learning source: ICLR 2019 method: environment: paper link: https://arxiv.org/pdf/1901.08162v1.pdf code: https://github.com/kantneel/causal-metarl interpretation: Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours source: arXiv:1812.09113 preprint 2019 method: environment: paper link: https://arxiv.org/pdf/1812.09113v3.pdf code: https://github.com/nvecoven/nmd_net interpretation: Policy Gradient RL Algorithms as Directed Acyclic Graphs source: arXiv:2012.07763 preprint 2020 method: environment: paper link: https://arxiv.org/pdf/2012.07763v2.pdf code: https://github.com/jjgarau/DAGPolicyGradient interpretation: Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems source: GECCO 2020 method: environment: paper link: https://arxiv.org/pdf/2004.12846v2.pdf code: https://github.com/dlpbc/penn-a interpretation: Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads source: RA-L 2021 method: environment: paper link: https://arxiv.org/pdf/2004.11345v2.pdf code: https://github.com/suneelbelkhale/model-based-meta-rl-for-flight interpretation: Hierarchical Meta Reinforcement Learning for Multi-Task Environments source: ICLR 2021 method: environment: paper link: https://openreview.net/pdf?id=u9ax42K7ND code: https://github.com/MeSH-ICLR/MEtaSoftHierarchy interpretation: Modeling and Optimization Trade-off in Meta-learning source: NeurIPS 2020 method: environment: paper link: https://arxiv.org/pdf/2010.12916v2.pdf code: https://github.com/intel-isl/MetaLearningTradeoffs interpretation: Meta-Learning of Structured Task Distributions in Humans and Machines source: ICLR 2021 method: environment: paper link: https://arxiv.org/pdf/2010.02317v3.pdf code: https://github.com/sreejank/Compositional_MetaRL interpretation: Offline Meta Learning of Exploration source: arXiv:2008.02598 preprint 2021 method: environment: paper link: https://arxiv.org/pdf/2008.02598v3.pdf code: https://github.com/Rondorf/BOReL interpretation: Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks source: ICC 2021 method: environment: paper link: https://arxiv.org/pdf/2102.12277v1.pdf code: https://github.com/wyy0206/THzVR interpretation:</summary></entry><entry><title type="html">Tips on How To Succeed in an ECE Major(如何在电子和计算机工程专业取得成功？)</title><link href="http://localhost:4000/2020/12/23/success.html" rel="alternate" type="text/html" title="Tips on How To Succeed in an ECE Major(如何在电子和计算机工程专业取得成功？)" /><published>2020-12-23T00:00:00+08:00</published><updated>2020-12-23T00:00:00+08:00</updated><id>http://localhost:4000/2020/12/23/success</id><content type="html" xml:base="http://localhost:4000/2020/12/23/success.html">&lt;blockquote&gt;
  &lt;p&gt;转自Reddit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;给还在迷茫的自动化、计算机、电子系的本科小朋友，祝你们早日找到目标，努力学习，打好基础。估计做到这些，特奖就离你们不远了orz.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tips on How To Succeed in an ECE Major (or any degree)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do the homework.&lt;/strong&gt; Even if it’s not required to turn in. It’s the only way to learn this ECE stuff. Don’t cheat on the homework, because when the exam comes around the lack of actual practice will show and you will do poorly. This is how a lot of my friends failed ECE classes. They just copied solutions for the homeworks, got all A’s on the homeworks, then failed the exams and the class. The solutions can be helpful if you use them to work through problems and learn, but they can be very hurtful if you use them wrong.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Work the hard problems.&lt;/strong&gt; Once you get really good at a certain type of problem, start working on a different type of problem. It’s easy to keep doing the problems you are good at and skip the hard ones. This is the opposite of what you should be doing! Work on the hard problems so you get practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Treat the school day like a work day.&lt;/strong&gt; Regardless of when your classes are, get to school at 8:00 AM, and stay until 5:00 PM. If you have an exam or something the next day, then stay longer. Work hard during that time to finish all your homework, studying, office hours, emails, etc. That way your evenings could possibly be free to get rest or have some fun!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to the professor’s office hours.&lt;/strong&gt; They are different people in their office than in front of class. They will answer your questions in a direct way. You will need letters of recommendation at some point in your life (applying for grad school, scholarships, jobs) and it is part of the professor’s job to write letters of rec for students. If you know them even a little bit, like by going to a few office hours, then they will gladly write you a great letter of rec later in life.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to the TA office hours.&lt;/strong&gt; They are usually awesome and will help you work through problems, and sometimes give you the solutions. They are students, too, so they understand what you are dealing with and may be able to explain things in a different way than the professors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make the circuit on a breadboard or in a simulator.&lt;/strong&gt; Nothing beats learning the concepts then actually putting them into practice. It is infinitely helpful to make the circuits from homework or the textbook examples with an actual breadboard. Remember to engineer hardware you have to understand the hardware, and the best way to do that is play with the actual hardware. Components are cheap on Amazon, or there are some free circuit simulators if you want. I personally used everycircuit even though it is paid, it is worth it for the visualizations. Another browser based one is PartSim. Here is a list of free simulators.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All-night cramming study sessions are hurtful.&lt;/strong&gt; They will not help you in an engineering degree. Don’t do them. You will need rest before an exam so you can think critically and your brain can have better recall. Sleep is key! To achieve no all-nighters, you have to start working on assignments early, and start studying early. Spreading out the studying over multiple days is helpful if you can do it early enough.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use the campus tutoring resources.&lt;/strong&gt; A lot of schools or departments offer free/cheap tutoring services. All of them will have some sort of resources. Ask your department counselor or admin person, they will point you in the right direction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Awesome with no effort” doesn’t happen in college.&lt;/strong&gt; You will have to make a deliberate effort to learn the material. You will have to make time to study and do homework. You will work your ass off. In high school or other degrees it is possible to rely on your smarts and just cruise through. That is not possible anymore, you gotta put in work to do well. Just being smart doesn’t count anymore, you have to have discipline and a work ethic. Luckily, those can be learned!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start working on assignments based on the assigned date, not the due date.&lt;/strong&gt; Don’t treat due dates like you did in high school, etc. Instead of planning assignments from the due date backwards, start doing work from the assigned date forwards. This is a big one! For example: If a homework is assigned on 11/5 but isn’t due until 11/20, don’t plan on doing the homework on 11/18. Instead, plan on doing the homework on 11/6. You could even trick yourself by writing down that the homework is due a few days before it actually is. That way you are done early and aren’t stressed out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start studying for exams a few (2-3) weeks before they come.&lt;/strong&gt; If you start studying the week before you will be stressed, not cover enough material, and struggle. Help yourself out and start early. It feels amazing when the exam is a day or two away and you already are done studying for it. Now any more studying is just an easy review, which makes you feel more confident, which makes you do better on the exam.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to each lecture.&lt;/strong&gt; If learning and excelling isn’t enough motivation, then think of the cost. You are throwing hundreds of dollars into the trash every time you skip a class. Just go. Even if you are sleepy or don’t want to. It’s absolutely critical to get in this habit.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Take notes by hand on paper, sit in the front rows, and don’t be afraid to ask questions.&lt;/strong&gt; You are at school, asking questions is part of learning! If the professor thought you didn’t have any questions, then what would be the purpose of school? Ask questions!(注：ipad记笔记体验也不错:))&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Take notes in your own words.&lt;/strong&gt; Simply copying down what the professor says word for word is fine for reference, but is not that helpful for learning. The textbook can be your reference. It is better to take notes in your own words, it forces you to process the information and write it in a new way, which increases retention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do the reading before the lecture.&lt;/strong&gt; I personally only did this a few times, but each time I did I felt like a freaking genius during the lecture. Even if I didn’t really get what I was reading, just being exposed to the terms and concepts made the lecture so much more effective.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to review your notes after the lecture.&lt;/strong&gt; What good are notes if you don’t look at them again?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find a good quiet place to study and make it your regular spot.&lt;/strong&gt; I prefer top floors of libraries or basements. Headphones are great with some homework music on. Gotta find a zone that works for you and get into it. Sometimes music is distracting, and white noise would be better. I prefer brown noise. Here’s a website that does both!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find a good time of day to study.&lt;/strong&gt; If you like studying at certain times of the day, then plan those times for studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make the most of your study time.&lt;/strong&gt; It’s easy to sit at a desk for 8 hours straight then say “I just studied for 8 hours.” In my experience, that’s not real studying. Most likely half or two thirds of that time was actually spent studying. Get the studying done with, then move on with your day, otherwise you will get burned out. Sometimes forcing small breaks and chunking your study time helps, like in the Pomodoro technique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Study one class at a time.&lt;/strong&gt; Context switching is an expensive mental operation. Meaning it wastes time to switch between classes. When studying or doing homework try to finish one thing first, then move on. It will feel good to get something off your plate as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ditch the laptop and phone when studying if you can.&lt;/strong&gt; Try to study with just a textbook, calculator, notebook, and pencil. It’s easy to get overwhelmed with homework and just browse reddit/facebook/internet for hours instead. Or play computer games. The biggest thing that ever helped me was studying without internet enabled electronic devices. It also helps to put a timer on your internet usage, so it makes your quick Google searches more urgent and less prone to wandering.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to find a study group with 2-5 students.&lt;/strong&gt; It really helps to have different people to study with, because everyone sees things slightly different. Plus it’s harder to slack off and play computer games when you have people sitting next to you. Too many people in a study group means that it becomes a social event, and no work gets done. I was very shy and didn’t get a study group until my last year of undergrad. It was just 2 people that I randomly asked “hey wanna study”. They will help you and you will help them and you will all be smarter. And you will feel social. Win win!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Teach someone else the material.&lt;/strong&gt; Saying the concepts out loud and forcing your brain to make sense of what you’re saying is a great way to learn the material. Yes, teaching helps you learn, it’s proven scientifically! You can do this with a classmate, friend, random person, or a rubber duck.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn the material from a different resource on your own.&lt;/strong&gt; Watch YouTube videos, MIT OCW videos, read Wikipedia articles, look for animations, do anything to learn the material in a different way. I highly recommend visualizations of circuits, EM fields, computer architecture, etc. Any of the complex things that will be learned in an ECE degree can be much more readily understood with a good diagram or animation. Here’s some personal favorites: Falstad circuit examples and an amazing Electric Circuits YouTube playlist&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use tools to supplement learning, but don’t abuse them.&lt;/strong&gt; Chegg step-by-step solutions, Wolfram Mathematica, etc. are very useful tools that can help you understand how to work through problems. Just remember that they are tools you can use to increase your knowledge. Don’t abuse them, or it will show on your exams and in your job interviews.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Have pride in what you are turning in.&lt;/strong&gt; Make your circuits look nice, check your spelling, format things nicely. It makes you feel good and makes you put more effort into the whole assignment, which ultimately gives you a better grade.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Understanding the foundations is a key to success.&lt;/strong&gt; This isn’t simply a memorization degree. You must really understand what’s going on at the basic physical level. You will excel in all the other things if you can get a handle on the basics (ie. Kirchhoff’s laws, Ohm’s laws, EM fields, basic C programming, calculus, statistics).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do something non-engineering to keep yourself sane.&lt;/strong&gt; Sketch, read sci-fi books, meditate, go on walks, practice piano, play Frisbee, something to balance out all the left brain activity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The only way to learn is to do problems.&lt;/strong&gt; Certain degrees can be got by rote memorization. Engineering degrees can only be got by critical thinking and actually doing. Do the problems. Do them multiple ways. Make the circuit on a breadboard or in a simulator. Use your hands, use your brain!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Find practice problems, homework from other schools/classes, textbook solutions, etc.&lt;/strong&gt; These are excellent resources for doing problems and learning. Be careful with solutions though!! It is easy to feel confident about a problem when doing it with solutions. That same problem on an exam will feel impossible because you don’t have solutions to look at. I like to use solutions to get me unstuck or to confirm I did the right step.(注：非常重要！！！国内的课程无论从lecture的内容丰富度、新鲜度还是实验的完善度、难度远没有国外的如MIT、伯克利等学校完善，学会利用国外学校的免费教育资源！)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google for previous classes to find old lectures, problems, etc.&lt;/strong&gt; If you are taking ECE 165 at UCSB, just google “ece 165 ucsb” or “ece 165 ucsb 2015” to find previous quarter’s class websites. They usually have old homeworks, lectures, solutions, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When studying for exams, make equation cheat sheets.&lt;/strong&gt; Distill all the complex equations, terms, conversions, etc. onto 1-2 pages. Do this even if your class does not allow them during the exam, just making them is a great study practice as it forces you to think about each important thing before an exam. Plus it’s a great resource later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make a “roadmap” before studying.&lt;/strong&gt; Usually this can be found in the syllabus. Basically you want to make a list of the topics you will learn so that you can quantify the amount of work you will be doing during the course. This makes your life much easier when studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make a “toolbox” after/during studying.&lt;/strong&gt; I like to take the key concepts from each chapter or topic and distill it to a single page of graphs, circuits, equations, terms, etc. This becomes a great reference and is a good exercise to parse the large amount of information you are learning into the important bits. Most courses are a bit like gold mining, you have to sift through and find the critical nuggets of information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At some point you will do poorly on an exam, homework, class, etc.&lt;/strong&gt; Don’t worry! Just take it as a sign that you need to devote more time and effort to the material. You know what they call medical students who get C’s? Doctor. You should try your best and take pride in your performance, but if you get hit with a bad grade don’t get down, just study harder next time!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use a planner or homework tracker to keep yourself on schedule.&lt;/strong&gt; There are a lot of assignments and you would be crazy to try to memorize it all. Just get a nice planner, or use Google Calendar, whatever. Schedule your classes, homework, and (importantly) time for studying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get connected on LinkedIn.&lt;/strong&gt; Connect with your peers and professor on LinkedIn! It’s a very useful tool to stay connected in a professional environment and the people hiring at companies always look up applicants on LinkedIn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn to teach yourself.&lt;/strong&gt; A degree in ECE is partly about learning the specific material, but it is mostly about learning critical thinking and how to teach yourself. These are the two big things that you will use for the rest of your life. At your future job you will have tools that do a lot of the calculations and work for you, but you must know how to teach yourself to use them and to think critically about the applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remember, this is a hard major.&lt;/strong&gt; So you gotta work hard. It will be a stressful challenge, but you can make your life easier by being smart about how you do the work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remember, this is a rewarding major.&lt;/strong&gt; ECE will pay off! Jobs are often exciting and interesting, and pay well. Here’s some statistics and information: Glassdoor salary estimates, Bureau of Labor Statistics employment stats and occupational outlook&lt;/p&gt;

&lt;p&gt;评论区里的补充：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Go to your career fairs on campus each semester if they happen that frequently.&lt;/strong&gt; Try talking with each of the representatives and find out what kind of classes and what kind of work they are doing. You get to practice your communication skills and also learn more about the industry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;textbooks are expensive.&lt;/strong&gt; if you are broke and are savvy with sketchy parts of the web, you can find most of the textbooks as PDF’s online using the guide at the link below. Virus scan everything and be smart about what you download. Piracy is bad, mmkay, buyer beware!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原始链接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&quot;reddit-card&quot; data-card-created=&quot;1608721895&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/ECE/comments/52rtx6/tips_on_how_to_succeed_in_an_ece_major/&quot;&gt;Tips on How To Succeed in an ECE Major&lt;/a&gt; from &lt;a href=&quot;http://www.reddit.com/r/ECE&quot;&gt;r/ECE&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//embed.redditmedia.com/widgets/platform.js&quot; charset=&quot;UTF-8&quot;&gt;&lt;/script&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="随感" /><summary type="html">转自Reddit. 给还在迷茫的自动化、计算机、电子系的本科小朋友，祝你们早日找到目标，努力学习，打好基础。估计做到这些，特奖就离你们不远了orz. Tips on How To Succeed in an ECE Major (or any degree) Do the homework. Even if it’s not required to turn in. It’s the only way to learn this ECE stuff. Don’t cheat on the homework, because when the exam comes around the lack of actual practice will show and you will do poorly. This is how a lot of my friends failed ECE classes. They just copied solutions for the homeworks, got all A’s on the homeworks, then failed the exams and the class. The solutions can be helpful if you use them to work through problems and learn, but they can be very hurtful if you use them wrong. Work the hard problems. Once you get really good at a certain type of problem, start working on a different type of problem. It’s easy to keep doing the problems you are good at and skip the hard ones. This is the opposite of what you should be doing! Work on the hard problems so you get practice. Treat the school day like a work day. Regardless of when your classes are, get to school at 8:00 AM, and stay until 5:00 PM. If you have an exam or something the next day, then stay longer. Work hard during that time to finish all your homework, studying, office hours, emails, etc. That way your evenings could possibly be free to get rest or have some fun! Go to the professor’s office hours. They are different people in their office than in front of class. They will answer your questions in a direct way. You will need letters of recommendation at some point in your life (applying for grad school, scholarships, jobs) and it is part of the professor’s job to write letters of rec for students. If you know them even a little bit, like by going to a few office hours, then they will gladly write you a great letter of rec later in life. Go to the TA office hours. They are usually awesome and will help you work through problems, and sometimes give you the solutions. They are students, too, so they understand what you are dealing with and may be able to explain things in a different way than the professors. Make the circuit on a breadboard or in a simulator. Nothing beats learning the concepts then actually putting them into practice. It is infinitely helpful to make the circuits from homework or the textbook examples with an actual breadboard. Remember to engineer hardware you have to understand the hardware, and the best way to do that is play with the actual hardware. Components are cheap on Amazon, or there are some free circuit simulators if you want. I personally used everycircuit even though it is paid, it is worth it for the visualizations. Another browser based one is PartSim. Here is a list of free simulators. All-night cramming study sessions are hurtful. They will not help you in an engineering degree. Don’t do them. You will need rest before an exam so you can think critically and your brain can have better recall. Sleep is key! To achieve no all-nighters, you have to start working on assignments early, and start studying early. Spreading out the studying over multiple days is helpful if you can do it early enough. Use the campus tutoring resources. A lot of schools or departments offer free/cheap tutoring services. All of them will have some sort of resources. Ask your department counselor or admin person, they will point you in the right direction. “Awesome with no effort” doesn’t happen in college. You will have to make a deliberate effort to learn the material. You will have to make time to study and do homework. You will work your ass off. In high school or other degrees it is possible to rely on your smarts and just cruise through. That is not possible anymore, you gotta put in work to do well. Just being smart doesn’t count anymore, you have to have discipline and a work ethic. Luckily, those can be learned! Start working on assignments based on the assigned date, not the due date. Don’t treat due dates like you did in high school, etc. Instead of planning assignments from the due date backwards, start doing work from the assigned date forwards. This is a big one! For example: If a homework is assigned on 11/5 but isn’t due until 11/20, don’t plan on doing the homework on 11/18. Instead, plan on doing the homework on 11/6. You could even trick yourself by writing down that the homework is due a few days before it actually is. That way you are done early and aren’t stressed out. Start studying for exams a few (2-3) weeks before they come. If you start studying the week before you will be stressed, not cover enough material, and struggle. Help yourself out and start early. It feels amazing when the exam is a day or two away and you already are done studying for it. Now any more studying is just an easy review, which makes you feel more confident, which makes you do better on the exam. Go to each lecture. If learning and excelling isn’t enough motivation, then think of the cost. You are throwing hundreds of dollars into the trash every time you skip a class. Just go. Even if you are sleepy or don’t want to. It’s absolutely critical to get in this habit. Take notes by hand on paper, sit in the front rows, and don’t be afraid to ask questions. You are at school, asking questions is part of learning! If the professor thought you didn’t have any questions, then what would be the purpose of school? Ask questions!(注：ipad记笔记体验也不错:)) Take notes in your own words. Simply copying down what the professor says word for word is fine for reference, but is not that helpful for learning. The textbook can be your reference. It is better to take notes in your own words, it forces you to process the information and write it in a new way, which increases retention. Do the reading before the lecture. I personally only did this a few times, but each time I did I felt like a freaking genius during the lecture. Even if I didn’t really get what I was reading, just being exposed to the terms and concepts made the lecture so much more effective. Try to review your notes after the lecture. What good are notes if you don’t look at them again? Find a good quiet place to study and make it your regular spot. I prefer top floors of libraries or basements. Headphones are great with some homework music on. Gotta find a zone that works for you and get into it. Sometimes music is distracting, and white noise would be better. I prefer brown noise. Here’s a website that does both! Find a good time of day to study. If you like studying at certain times of the day, then plan those times for studying. Make the most of your study time. It’s easy to sit at a desk for 8 hours straight then say “I just studied for 8 hours.” In my experience, that’s not real studying. Most likely half or two thirds of that time was actually spent studying. Get the studying done with, then move on with your day, otherwise you will get burned out. Sometimes forcing small breaks and chunking your study time helps, like in the Pomodoro technique. Study one class at a time. Context switching is an expensive mental operation. Meaning it wastes time to switch between classes. When studying or doing homework try to finish one thing first, then move on. It will feel good to get something off your plate as well. Ditch the laptop and phone when studying if you can. Try to study with just a textbook, calculator, notebook, and pencil. It’s easy to get overwhelmed with homework and just browse reddit/facebook/internet for hours instead. Or play computer games. The biggest thing that ever helped me was studying without internet enabled electronic devices. It also helps to put a timer on your internet usage, so it makes your quick Google searches more urgent and less prone to wandering. Try to find a study group with 2-5 students. It really helps to have different people to study with, because everyone sees things slightly different. Plus it’s harder to slack off and play computer games when you have people sitting next to you. Too many people in a study group means that it becomes a social event, and no work gets done. I was very shy and didn’t get a study group until my last year of undergrad. It was just 2 people that I randomly asked “hey wanna study”. They will help you and you will help them and you will all be smarter. And you will feel social. Win win! Teach someone else the material. Saying the concepts out loud and forcing your brain to make sense of what you’re saying is a great way to learn the material. Yes, teaching helps you learn, it’s proven scientifically! You can do this with a classmate, friend, random person, or a rubber duck. Learn the material from a different resource on your own. Watch YouTube videos, MIT OCW videos, read Wikipedia articles, look for animations, do anything to learn the material in a different way. I highly recommend visualizations of circuits, EM fields, computer architecture, etc. Any of the complex things that will be learned in an ECE degree can be much more readily understood with a good diagram or animation. Here’s some personal favorites: Falstad circuit examples and an amazing Electric Circuits YouTube playlist Use tools to supplement learning, but don’t abuse them. Chegg step-by-step solutions, Wolfram Mathematica, etc. are very useful tools that can help you understand how to work through problems. Just remember that they are tools you can use to increase your knowledge. Don’t abuse them, or it will show on your exams and in your job interviews. Have pride in what you are turning in. Make your circuits look nice, check your spelling, format things nicely. It makes you feel good and makes you put more effort into the whole assignment, which ultimately gives you a better grade. Understanding the foundations is a key to success. This isn’t simply a memorization degree. You must really understand what’s going on at the basic physical level. You will excel in all the other things if you can get a handle on the basics (ie. Kirchhoff’s laws, Ohm’s laws, EM fields, basic C programming, calculus, statistics). Do something non-engineering to keep yourself sane. Sketch, read sci-fi books, meditate, go on walks, practice piano, play Frisbee, something to balance out all the left brain activity. The only way to learn is to do problems. Certain degrees can be got by rote memorization. Engineering degrees can only be got by critical thinking and actually doing. Do the problems. Do them multiple ways. Make the circuit on a breadboard or in a simulator. Use your hands, use your brain! Find practice problems, homework from other schools/classes, textbook solutions, etc. These are excellent resources for doing problems and learning. Be careful with solutions though!! It is easy to feel confident about a problem when doing it with solutions. That same problem on an exam will feel impossible because you don’t have solutions to look at. I like to use solutions to get me unstuck or to confirm I did the right step.(注：非常重要！！！国内的课程无论从lecture的内容丰富度、新鲜度还是实验的完善度、难度远没有国外的如MIT、伯克利等学校完善，学会利用国外学校的免费教育资源！) Google for previous classes to find old lectures, problems, etc. If you are taking ECE 165 at UCSB, just google “ece 165 ucsb” or “ece 165 ucsb 2015” to find previous quarter’s class websites. They usually have old homeworks, lectures, solutions, etc. When studying for exams, make equation cheat sheets. Distill all the complex equations, terms, conversions, etc. onto 1-2 pages. Do this even if your class does not allow them during the exam, just making them is a great study practice as it forces you to think about each important thing before an exam. Plus it’s a great resource later. Make a “roadmap” before studying. Usually this can be found in the syllabus. Basically you want to make a list of the topics you will learn so that you can quantify the amount of work you will be doing during the course. This makes your life much easier when studying. Make a “toolbox” after/during studying. I like to take the key concepts from each chapter or topic and distill it to a single page of graphs, circuits, equations, terms, etc. This becomes a great reference and is a good exercise to parse the large amount of information you are learning into the important bits. Most courses are a bit like gold mining, you have to sift through and find the critical nuggets of information. At some point you will do poorly on an exam, homework, class, etc. Don’t worry! Just take it as a sign that you need to devote more time and effort to the material. You know what they call medical students who get C’s? Doctor. You should try your best and take pride in your performance, but if you get hit with a bad grade don’t get down, just study harder next time! Use a planner or homework tracker to keep yourself on schedule. There are a lot of assignments and you would be crazy to try to memorize it all. Just get a nice planner, or use Google Calendar, whatever. Schedule your classes, homework, and (importantly) time for studying. Get connected on LinkedIn. Connect with your peers and professor on LinkedIn! It’s a very useful tool to stay connected in a professional environment and the people hiring at companies always look up applicants on LinkedIn. Learn to teach yourself. A degree in ECE is partly about learning the specific material, but it is mostly about learning critical thinking and how to teach yourself. These are the two big things that you will use for the rest of your life. At your future job you will have tools that do a lot of the calculations and work for you, but you must know how to teach yourself to use them and to think critically about the applications. Remember, this is a hard major. So you gotta work hard. It will be a stressful challenge, but you can make your life easier by being smart about how you do the work. Remember, this is a rewarding major. ECE will pay off! Jobs are often exciting and interesting, and pay well. Here’s some statistics and information: Glassdoor salary estimates, Bureau of Labor Statistics employment stats and occupational outlook 评论区里的补充： Go to your career fairs on campus each semester if they happen that frequently. Try talking with each of the representatives and find out what kind of classes and what kind of work they are doing. You get to practice your communication skills and also learn more about the industry. textbooks are expensive. if you are broke and are savvy with sketchy parts of the web, you can find most of the textbooks as PDF’s online using the guide at the link below. Virus scan everything and be smart about what you download. Piracy is bad, mmkay, buyer beware! 原始链接 Tips on How To Succeed in an ECE Major from r/ECE</summary></entry><entry><title type="html">cuFAST:基于GPU的FAST算法加速</title><link href="http://localhost:4000/2020/12/19/pdf-cufast.html" rel="alternate" type="text/html" title="cuFAST:基于GPU的FAST算法加速" /><published>2020-12-19T00:00:00+08:00</published><updated>2020-12-19T00:00:00+08:00</updated><id>http://localhost:4000/2020/12/19/pdf-cufast</id><content type="html" xml:base="http://localhost:4000/2020/12/19/pdf-cufast.html">&lt;center&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:718px; height:700px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;/web/viewer.html?file=/pdf/CAC2020.pdf&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:500px; height:100px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/tianyma/tianyma.github.io/blob/main/pdf/CAC2020.pdf&quot; class=&quot;image fit&quot; &gt;&lt;img src=&quot;images/marr_pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt; --&gt;
&lt;a href=&quot;/pdf.js/web/viewer.html?file=/pdf/cufast.pdf&quot;&gt;Click to view this blog.&lt;/a&gt;

&lt;!-- &lt;object data=&quot;/pdf/CAC2020.pdf&quot; width=&quot;1000&quot; height=&quot;1000&quot; type='application/pdf'/&gt; --&gt;
&lt;/center&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="cuda" /><summary type="html">Click to view this blog.</summary></entry><entry><title type="html">CAC2020(for pdf view test)</title><link href="http://localhost:4000/2020/11/26/pdf-test.html" rel="alternate" type="text/html" title="CAC2020(for pdf view test)" /><published>2020-11-26T00:00:00+08:00</published><updated>2020-11-26T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/26/pdf-test</id><content type="html" xml:base="http://localhost:4000/2020/11/26/pdf-test.html">&lt;center&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:718px; height:700px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;/web/viewer.html?file=/pdf/CAC2020.pdf&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;iframe src=&quot;http://docs.google.com/gview?url=/pdf/CAC2020.pdf&amp;embedded=true&quot; style=&quot;width:500px; height:100px;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
&lt;!-- &lt;a href=&quot;https://github.com/tianyma/tianyma.github.io/blob/main/pdf/CAC2020.pdf&quot; class=&quot;image fit&quot; &gt;&lt;img src=&quot;images/marr_pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt; --&gt;
&lt;a href=&quot;/pdf.js/web/viewer.html?file=/pdf/CAC2020.pdf&quot;&gt;Click to view this blog.&lt;/a&gt;

&lt;!-- &lt;object data=&quot;/pdf/CAC2020.pdf&quot; width=&quot;1000&quot; height=&quot;1000&quot; type='application/pdf'/&gt; --&gt;
&lt;/center&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><summary type="html">Click to view this blog.</summary></entry><entry><title type="html">使用docker运行多个版本的cuda</title><link href="http://localhost:4000/2020/11/21/docker-cuda.html" rel="alternate" type="text/html" title="使用docker运行多个版本的cuda" /><published>2020-11-21T00:00:00+08:00</published><updated>2020-11-21T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/21/docker-cuda</id><content type="html" xml:base="http://localhost:4000/2020/11/21/docker-cuda.html">&lt;blockquote&gt;
  &lt;p&gt;如果你遇到了这样一个问题:&lt;/p&gt;

  &lt;p&gt;我的机器上装的cuda版本是最新的，比如cuda 11.1，然而我拿到了一个在cuda 8.0的版本上编译好的程序，我该怎么运行它呢？总不会我又要安装一个8.0的cuda吧。(这个问题来自于我在做cmu 15-418 assignment task2时需要运行在cuda8.0环境编译的参考程序cudaScan_ref，而我的cuda是11.1版本)&lt;/p&gt;

  &lt;p&gt;这个时候你肯定立刻会想到虚拟化，作为轻便好用的虚拟化工具，docker就派上用场了。&lt;/p&gt;

  &lt;p&gt;这篇文章就来介绍一下如何使用docker运行多版本的cuda。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;本机环境
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;系统：18.04.5 LTS (Bionic Beaver)
已安装cuda版本：11.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h2 id=&quot;１-docker准备&quot;&gt;１ docker准备&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;11-了解原理和基本命令&quot;&gt;1.1 了解原理和基本命令&lt;/h3&gt;
&lt;p&gt;如果你还是个docker小白，建议先通过&lt;a href=&quot;https://docs.docker.com/get-started/overview/&quot;&gt;官方文档&lt;/a&gt;了解一下docker的原理和基本用法，如运行、停止、删除镜像等。&lt;/p&gt;

&lt;h3 id=&quot;12-安装&quot;&gt;1.2 安装&lt;/h3&gt;
&lt;p&gt;官方的&lt;a href=&quot;https://docs.docker.com/engine/install/&quot;&gt;安装教程&lt;/a&gt;提供了对应不同操作系统的安装方法。&lt;/p&gt;

&lt;p&gt;当然，由于ubuntu 18.04源已经集成了docker软件，可以直接通过包管理工具安装。方法是&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# on ubuntu 18.04
sudo apt instll docker.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;2-nvidia-container-toolkit准备&quot;&gt;2 Nvidia Container Toolkit准备&lt;/h2&gt;
&lt;p&gt;Nvidia目前提供&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Nvidia Container Toolkit&lt;/code&gt;和&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvidia-docker2&lt;/code&gt;包完成对docker的支持，可以通过&lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker&quot;&gt;安装教程&lt;/a&gt;进行安装。&lt;/p&gt;

&lt;p&gt;安装的时候要注意其支持的系统环境和需要的环境依赖。&lt;/p&gt;

&lt;h2 id=&quot;3-在docker上运行cuda环境&quot;&gt;3 在docker上运行cuda环境&lt;/h2&gt;
&lt;p&gt;准备工作完成后，就可以来解决文章开始提出的问题了:如何在host环境为cuda11.1的情况下使用docker运行基于cuda8.0编译的程序？&lt;/p&gt;

&lt;h3 id=&quot;31-镜像获取&quot;&gt;3.1 镜像获取&lt;/h3&gt;
&lt;p&gt;首先要从&lt;a href=&quot;https://hub.docker.com/&quot;&gt;docker-hub&lt;/a&gt;上找到我们需要的环境的镜像，使用关键字&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cuda8.0&lt;/code&gt;搜索即可，然后在本地拉取镜像源。&lt;/p&gt;

&lt;p&gt;比如：我在docker-hub上找到这样的镜像：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;klauskyj/cuda8.0-cudnn7
&lt;/code&gt;。那么就可以通过&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker pull klauskyj/cuda8.0-cudnn7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;拉取下来了。
通过&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker ps -a 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;可以看到自己拉取的镜像。&lt;/p&gt;
&lt;h3 id=&quot;32-运行镜像&quot;&gt;3.2 运行镜像&lt;/h3&gt;
&lt;p&gt;类似于普通镜像运行自己拉取下来的镜像，runtime设为nvidia即可，如&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker run -it --runtime=nvidia [DOCKER_IMAGE]:[DOCKER_TAG] /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;进入交互式环境后，去&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local&lt;/code&gt;目录下就可以看到cuda 8.0了。&lt;/p&gt;
&lt;h3 id=&quot;33-运行程序&quot;&gt;3.3 运行程序&lt;/h3&gt;
&lt;p&gt;那么如何在容器中运行程序呢？我们需要将程序拷贝进去。从主机拷贝文件到容器中步骤如下：&lt;/p&gt;

&lt;p&gt;1.通过&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker ps -a
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;获取容器的短ID或者name。&lt;/p&gt;

&lt;p&gt;2.通过&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;sudo docker inspect -f ‘{{ .Id }}’ [短ID or name]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;获取容器的完整ID。
如：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;➜  render git:(master) ✗ sudo docker inspect -f ‘{{ .Id }}’ cc1ec222c1ff
cc1ec222c1ff72272a00061aa197ab8ba13bd446697b9d9c1820c45ba16b75cd&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;3.通过&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker cp [本地文件路径] [ID全称]:[容器路径]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;拷贝文件，注意拷贝目录的话也一样，不用加其他参数。&lt;/p&gt;

&lt;p&gt;如果从容器中传输文件到host，通过&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker cp [ID全称]:[容器文件路径] [本地路径]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;完成。&lt;/p&gt;

&lt;p&gt;最后，在容器中就可以运行这个程序啦。&lt;/p&gt;

&lt;h2 id=&quot;4-总结&quot;&gt;4 总结&lt;/h2&gt;
&lt;p&gt;这篇文章其实介绍了基本的docker用法，nvidia提供的nvidia-docker2.0封装的很好，可以方便地建立cuda环境，这里要注意虚拟化的cuda的版本要和自己的GPU匹配。&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="docker" /><category term="cuda" /><summary type="html">如果你遇到了这样一个问题: 我的机器上装的cuda版本是最新的，比如cuda 11.1，然而我拿到了一个在cuda 8.0的版本上编译好的程序，我该怎么运行它呢？总不会我又要安装一个8.0的cuda吧。(这个问题来自于我在做cmu 15-418 assignment task2时需要运行在cuda8.0环境编译的参考程序cudaScan_ref，而我的cuda是11.1版本) 这个时候你肯定立刻会想到虚拟化，作为轻便好用的虚拟化工具，docker就派上用场了。 这篇文章就来介绍一下如何使用docker运行多版本的cuda。 本机环境 系统：18.04.5 LTS (Bionic Beaver) 已安装cuda版本：11.1 １ docker准备 1.1 了解原理和基本命令 如果你还是个docker小白，建议先通过官方文档了解一下docker的原理和基本用法，如运行、停止、删除镜像等。 1.2 安装 官方的安装教程提供了对应不同操作系统的安装方法。 当然，由于ubuntu 18.04源已经集成了docker软件，可以直接通过包管理工具安装。方法是 # on ubuntu 18.04 sudo apt instll docker.io 2 Nvidia Container Toolkit准备 Nvidia目前提供Nvidia Container Toolkit和nvidia-docker2包完成对docker的支持，可以通过安装教程进行安装。 安装的时候要注意其支持的系统环境和需要的环境依赖。 3 在docker上运行cuda环境 准备工作完成后，就可以来解决文章开始提出的问题了:如何在host环境为cuda11.1的情况下使用docker运行基于cuda8.0编译的程序？ 3.1 镜像获取 首先要从docker-hub上找到我们需要的环境的镜像，使用关键字cuda8.0搜索即可，然后在本地拉取镜像源。 比如：我在docker-hub上找到这样的镜像：klauskyj/cuda8.0-cudnn7 。那么就可以通过 sudo docker pull klauskyj/cuda8.0-cudnn7 拉取下来了。 通过 sudo docker ps -a 可以看到自己拉取的镜像。 3.2 运行镜像 类似于普通镜像运行自己拉取下来的镜像，runtime设为nvidia即可，如 sudo docker run -it --runtime=nvidia [DOCKER_IMAGE]:[DOCKER_TAG] /bin/bash 进入交互式环境后，去/usr/local目录下就可以看到cuda 8.0了。 3.3 运行程序 那么如何在容器中运行程序呢？我们需要将程序拷贝进去。从主机拷贝文件到容器中步骤如下： 1.通过 sudo docker ps -a 获取容器的短ID或者name。 2.通过 sudo docker inspect -f ‘{{ .Id }}’ [短ID or name] 获取容器的完整ID。 如： ➜ render git:(master) ✗ sudo docker inspect -f ‘{{ .Id }}’ cc1ec222c1ff cc1ec222c1ff72272a00061aa197ab8ba13bd446697b9d9c1820c45ba16b75cd 3.通过 sudo docker cp [本地文件路径] [ID全称]:[容器路径] 拷贝文件，注意拷贝目录的话也一样，不用加其他参数。 如果从容器中传输文件到host，通过 sudo docker cp [ID全称]:[容器文件路径] [本地路径] 完成。 最后，在容器中就可以运行这个程序啦。 4 总结 这篇文章其实介绍了基本的docker用法，nvidia提供的nvidia-docker2.0封装的很好，可以方便地建立cuda环境，这里要注意虚拟化的cuda的版本要和自己的GPU匹配。</summary></entry><entry><title type="html">CMU 15-418 assignment2 解题记录</title><link href="http://localhost:4000/2020/11/20/15-418-assignment2.html" rel="alternate" type="text/html" title="CMU 15-418 assignment2 解题记录" /><published>2020-11-20T00:00:00+08:00</published><updated>2020-11-20T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/20/15-418-assignment2</id><content type="html" xml:base="http://localhost:4000/2020/11/20/15-418-assignment2.html">&lt;p&gt;下面是CMU 15-418/15-618: Parallel Computer Architecture and Programming课程解题记录。&lt;/p&gt;

&lt;p&gt;本部分题解是assignment2，主题是cuda并行编程，这里是&lt;a href=&quot;http://www.cs.cmu.edu/~418/assignment_writeups/asst2.pdf&quot;&gt;题目&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;part-1-saxpysum-of-a-by-x-plus-y&quot;&gt;Part 1 SAXPY(Sum of A by X PLUS Y)&lt;/h2&gt;

&lt;h2 id=&quot;part-2-parallel-prefix-sum&quot;&gt;Part 2 Parallel Prefix-Sum&lt;/h2&gt;
&lt;p&gt;本部分要求分别并行求解前缀和和重复元素索引。关于什么是前缀和和重复元素索引题目中有详细说明，这里我分别记录一下解题思路。&lt;/p&gt;

&lt;p&gt;由于求解重复元素索引要用到求解前缀和的方法，所以先求解前缀和，在CPU版本中，题目代码中分别给出了并行版本和串行版本，所以GPU版本只需要对CPU的并行版本进行改写即可。&lt;/p&gt;

&lt;p&gt;求解重复元素索引的方法分为下面几个步骤：&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. 声明如下数组flag, prefixflag,input, output
2. 如果input[index] = input[index + 1], flag[index] = 1;
3. 对flag求前缀和得到prefixflag。
4. 重复元素个数count为prefixflag最后一位的值。
5. 如果prefixflag[index] = prefixflag[index - 1], output[prefixflag[index - 1]] = index - 1。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;举个例子:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;item&lt;/th&gt;
      &lt;th&gt;value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;input&lt;/td&gt;
      &lt;td&gt;[1,2,2,1,1,1,3,5,3,3]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;flag&lt;/td&gt;
      &lt;td&gt;[0,1,0,1,1,0,0,0,1,0]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;prefixflag&lt;/td&gt;
      &lt;td&gt;[0,0,1,1,2,3,3,3,3,4]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;output&lt;/td&gt;
      &lt;td&gt;[1,3,4,8]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="cmu15-418" /><summary type="html">下面是CMU 15-418/15-618: Parallel Computer Architecture and Programming课程解题记录。 本部分题解是assignment2，主题是cuda并行编程，这里是题目。 Part 1 SAXPY(Sum of A by X PLUS Y) Part 2 Parallel Prefix-Sum 本部分要求分别并行求解前缀和和重复元素索引。关于什么是前缀和和重复元素索引题目中有详细说明，这里我分别记录一下解题思路。 由于求解重复元素索引要用到求解前缀和的方法，所以先求解前缀和，在CPU版本中，题目代码中分别给出了并行版本和串行版本，所以GPU版本只需要对CPU的并行版本进行改写即可。 求解重复元素索引的方法分为下面几个步骤： 1. 声明如下数组flag, prefixflag,input, output 2. 如果input[index] = input[index + 1], flag[index] = 1; 3. 对flag求前缀和得到prefixflag。 4. 重复元素个数count为prefixflag最后一位的值。 5. 如果prefixflag[index] = prefixflag[index - 1], output[prefixflag[index - 1]] = index - 1。 举个例子: item value input [1,2,2,1,1,1,3,5,3,3] flag [0,1,0,1,1,0,0,0,1,0] prefixflag [0,0,1,1,2,3,3,3,3,4] output [1,3,4,8] count 4</summary></entry><entry><title type="html">Vim Cheat Sheet (不定期更新)</title><link href="http://localhost:4000/2020/11/18/basic-vim.html" rel="alternate" type="text/html" title="Vim Cheat Sheet (不定期更新)" /><published>2020-11-18T00:00:00+08:00</published><updated>2020-11-18T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/18/basic-vim</id><content type="html" xml:base="http://localhost:4000/2020/11/18/basic-vim.html">&lt;p&gt;下面用一个cheat sheet记录一些vim的快捷指令，方便自己需要的时候查询。&lt;/p&gt;
&lt;h2 id=&quot;search&quot;&gt;search&lt;/h2&gt;
&lt;p&gt;查找模式中加\c为大小写不敏感查找,\C为大小写敏感查找&lt;/p&gt;

&lt;p&gt;e.g.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ignore uppercase
/foo\c or 
:
# do not ignore uppercase
/foo\C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;replace&quot;&gt;replace&lt;/h2&gt;
&lt;p&gt;:s（substitute）命令用来查找和替换字符串。格式为&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# the partition character can be '/' or '#' or '@' (maybe other special characters)
:{作用范围}s/{目标}/{替换}/{替换标志} or 
:{作用范围}s#{目标}#{替换}#{替换标志} or
:{作用范围}s@{目标}@{替换}@{替换标志}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;其中,替换标志有下面几种:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;替换标志&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
      &lt;th&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;i/I&lt;/td&gt;
      &lt;td&gt;大小写敏感查询&lt;/td&gt;
      &lt;td&gt;i为敏感&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;c&lt;/td&gt;
      &lt;td&gt;替换前需要确认&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g&lt;/td&gt;
      &lt;td&gt;全局替换&lt;/td&gt;
      &lt;td&gt;一行中的多个匹配项都会被替换&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# replace foo with bar in current line for all matches
:s/foo/bar/g

# replace foo with bar in the whole context only for the first match of each line
:%s/foo/bar/

# replace foo with bar searching from line 5 to line 12
:5,12s/foo/bar/

# replace foo with bar searching from current line to the 2 plus lines(3 lines in all) in uppercase ignore case
:.,+2s/foo/bar/I
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="工具" /><summary type="html">下面用一个cheat sheet记录一些vim的快捷指令，方便自己需要的时候查询。 search 查找模式中加\c为大小写不敏感查找,\C为大小写敏感查找 e.g. # ignore uppercase /foo\c or : # do not ignore uppercase /foo\C replace :s（substitute）命令用来查找和替换字符串。格式为 # the partition character can be '/' or '#' or '@' (maybe other special characters) :{作用范围}s/{目标}/{替换}/{替换标志} or :{作用范围}s#{目标}#{替换}#{替换标志} or :{作用范围}s@{目标}@{替换}@{替换标志} 其中,替换标志有下面几种: 替换标志 含义 备注 i/I 大小写敏感查询 i为敏感 c 替换前需要确认   g 全局替换 一行中的多个匹配项都会被替换 e.g. # replace foo with bar in current line for all matches :s/foo/bar/g # replace foo with bar in the whole context only for the first match of each line :%s/foo/bar/ # replace foo with bar searching from line 5 to line 12 :5,12s/foo/bar/ # replace foo with bar searching from current line to the 2 plus lines(3 lines in all) in uppercase ignore case :.,+2s/foo/bar/I</summary></entry></feed>