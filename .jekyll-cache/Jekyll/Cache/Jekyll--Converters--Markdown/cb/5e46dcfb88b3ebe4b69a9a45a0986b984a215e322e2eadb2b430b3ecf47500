I"v	<h2 id="核心思想">核心思想</h2>
<p>MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 <br />
MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。</p>
<h2 id="实现过程">实现过程</h2>
<p>以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。  <br />
由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。</p>
<h2 id="测试任务">测试任务</h2>
<p>在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。</p>
<h2 id="创新点">创新点</h2>
<p>MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。</p>
<h2 id="算法评价">算法评价</h2>
<p>MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。</p>

<p><a href="https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html">返回文章列表</a></p>
:ET