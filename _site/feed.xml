<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2021-06-11T15:08:17+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">tianyma’s blog</title><subtitle>do valuable research, be valuable person.
</subtitle><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><entry><title type="html">Paper reading: BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning</title><link href="http://localhost:4000/2021/06/09/bisnn.html" rel="alternate" type="text/html" title="Paper reading: BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning" /><published>2021-06-09T00:00:00+08:00</published><updated>2021-06-09T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/09/bisnn</id><content type="html" xml:base="http://localhost:4000/2021/06/09/bisnn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;SNN的神经元（即激活函数）输出的是脉冲，也就是二进制值，这使得SNN在训练和推理的计算量上相比于ANN降低了很多。同时，如果将权重进行量化，可以进一步降低计算量，其中一种量化后的SNN为BiSNN(Binary SNN)，BiSNN的权重为1 bit(+1 or -1)。  &lt;br /&gt;
但是，SNN也存在着由于激活函数不可微分而难以训练的问题。大部分工作的做法是使用近似梯度通过时序反向传播训练。本文针对BiSNN(binary SNN)提出了Bayes-BiSNN训练算法，可以利用梯度下降直接训练BiSNN。同时，本文将直通滤波器和近似梯度结合起来，提出了一种对BiSNN的的训练算法ST-BiSNN。最后，本文对两种训练算法进行了对比。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;两种训练算法均针对监督学习设计，即输入为数据集的数据标签对和超参数，输出学习到的1 bit权重。BiSNN的权重初始化为实数值，在训练过程中，重复以下步骤直到算法收敛。具体的，从数据集中选出一个序列输入，首先对权重二值化，然后利用监督学习的反向传播的训练范式进行梯度下降，其中，梯度计算利用的是二值化的权重，并用计算的梯度更新实数值权重。ST-BiSNN算法和Bayes-BiSNN的权重二值化过程、梯度计算过程和参数更新算法不同，但整体流程一样。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;两个训练算法分别在一维和二维数据上进行了测试，并最后在MNIST-DSV数据集上进行了测试。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文的创新点在于两个梯度更新算法的理论分析，尤其是Bayes-BiSNN，从理论上证明了可以通过反向传播直接训练BiSNN。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;ST-BiSNN和Bayes-BiSNN是两个直接对BiSNN进行训练的算法，相比于ANN-SNN转换方法，更加高效食用，同时也为SNN训练算法设计提供了一些思路。&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><category term="SNN" /><summary type="html">核心思想 SNN的神经元（即激活函数）输出的是脉冲，也就是二进制值，这使得SNN在训练和推理的计算量上相比于ANN降低了很多。同时，如果将权重进行量化，可以进一步降低计算量，其中一种量化后的SNN为BiSNN(Binary SNN)，BiSNN的权重为1 bit(+1 or -1)。 但是，SNN也存在着由于激活函数不可微分而难以训练的问题。大部分工作的做法是使用近似梯度通过时序反向传播训练。本文针对BiSNN(binary SNN)提出了Bayes-BiSNN训练算法，可以利用梯度下降直接训练BiSNN。同时，本文将直通滤波器和近似梯度结合起来，提出了一种对BiSNN的的训练算法ST-BiSNN。最后，本文对两种训练算法进行了对比。 实现过程 两种训练算法均针对监督学习设计，即输入为数据集的数据标签对和超参数，输出学习到的1 bit权重。BiSNN的权重初始化为实数值，在训练过程中，重复以下步骤直到算法收敛。具体的，从数据集中选出一个序列输入，首先对权重二值化，然后利用监督学习的反向传播的训练范式进行梯度下降，其中，梯度计算利用的是二值化的权重，并用计算的梯度更新实数值权重。ST-BiSNN算法和Bayes-BiSNN的权重二值化过程、梯度计算过程和参数更新算法不同，但整体流程一样。 测试任务 两个训练算法分别在一维和二维数据上进行了测试，并最后在MNIST-DSV数据集上进行了测试。 创新点 本文的创新点在于两个梯度更新算法的理论分析，尤其是Bayes-BiSNN，从理论上证明了可以通过反向传播直接训练BiSNN。 算法评价 ST-BiSNN和Bayes-BiSNN是两个直接对BiSNN进行训练的算法，相比于ANN-SNN转换方法，更加高效食用，同时也为SNN训练算法设计提供了一些思路。</summary></entry><entry><title type="html">Paper reading: A Simple Neural Attentive Meta-Learner</title><link href="http://localhost:4000/2021/06/08/snail.html" rel="alternate" type="text/html" title="Paper reading: A Simple Neural Attentive Meta-Learner" /><published>2021-06-08T00:00:00+08:00</published><updated>2021-06-08T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/08/snail</id><content type="html" xml:base="http://localhost:4000/2021/06/08/snail.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;SNAIL结合了temporal convolution和soft attention，是一个model-based meta learning算法。temporal convolution可以直接快速获取大量历史信息，soft attention可以以一种key-value的形式精确定位信息。两者的互补使得SNAIL既可以以高带宽访问历史信息，又可以精确定位到需要的信息。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;SNAIL通过在网络中交替使用temporal convolution layer和 soft attention layer实现。  &lt;br /&gt;
具体的，在监督任务中，在$1, …, t-1$时刻SNAIL依次输入(example, label)对$(x_1, y_1), …, (x_{t-1}, y_{t-1})$，之后输入一个无标签的example$(x_t, -)$，输出对$x_t$的标签的预测。  &lt;br /&gt;
在强化学习任务中，在$1, …, t-1$时刻SNAIL依次输入(observation, action, reward)对$(o_1, -, -), …, (o_t, a_{t-1}, r_{t-1})$，然后在$t$时刻输出对$a_t$的分布的预测。在整个学习过程中，跨episode的信息都会被保存下来，从而可以拥有不同episode的泛化能力。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文在监督学习和强化学习上都对SNAIL进行了测试，其中监督学习的任务是few-shot image classification，强化学习任务包括multi-armed bandits、Tabular MDPs、3D visual navigation(VizDoom)、Locomotion tasks(mujoco)。&lt;/p&gt;

&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文将temporal convolution layer和soft attention layer在网络中交替使用，使得二者可以互补，既可以实现对历史信息的高带宽访问，又可以实现精确查找。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;SNAIL是一个典型的model-based meta learning算法，本文对算法的描述思路很清晰，并分别在监督学习任务和强化学习任务上进行了测试。但是，除了模型设计外，meta learning还需要从任务的角度进行分析，这样才能获得更好的任务泛化能力，在lifelong learning这种任务上也有助于解决计算及存储复杂性的问题。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 SNAIL结合了temporal convolution和soft attention，是一个model-based meta learning算法。temporal convolution可以直接快速获取大量历史信息，soft attention可以以一种key-value的形式精确定位信息。两者的互补使得SNAIL既可以以高带宽访问历史信息，又可以精确定位到需要的信息。 实现过程 SNAIL通过在网络中交替使用temporal convolution layer和 soft attention layer实现。 具体的，在监督任务中，在$1, …, t-1$时刻SNAIL依次输入(example, label)对$(x_1, y_1), …, (x_{t-1}, y_{t-1})$，之后输入一个无标签的example$(x_t, -)$，输出对$x_t$的标签的预测。 在强化学习任务中，在$1, …, t-1$时刻SNAIL依次输入(observation, action, reward)对$(o_1, -, -), …, (o_t, a_{t-1}, r_{t-1})$，然后在$t$时刻输出对$a_t$的分布的预测。在整个学习过程中，跨episode的信息都会被保存下来，从而可以拥有不同episode的泛化能力。 测试任务 本文在监督学习和强化学习上都对SNAIL进行了测试，其中监督学习的任务是few-shot image classification，强化学习任务包括multi-armed bandits、Tabular MDPs、3D visual navigation(VizDoom)、Locomotion tasks(mujoco)。 创新点 本文将temporal convolution layer和soft attention layer在网络中交替使用，使得二者可以互补，既可以实现对历史信息的高带宽访问，又可以实现精确查找。 算法评价 SNAIL是一个典型的model-based meta learning算法，本文对算法的描述思路很清晰，并分别在监督学习任务和强化学习任务上进行了测试。但是，除了模型设计外，meta learning还需要从任务的角度进行分析，这样才能获得更好的任务泛化能力，在lifelong learning这种任务上也有助于解决计算及存储复杂性的问题。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Prefrontal cortex as a meta-reinforcement learning system</title><link href="http://localhost:4000/2021/06/07/prefrontal.html" rel="alternate" type="text/html" title="Paper reading: Prefrontal cortex as a meta-reinforcement learning system" /><published>2021-06-07T00:00:00+08:00</published><updated>2021-06-07T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/07/prefrontal</id><content type="html" xml:base="http://localhost:4000/2021/06/07/prefrontal.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;传统的强化学习理论的生物学基础是通过多巴胺传递奖励误差信号，从而动态调整神经突触的连接，来进行学习，达到最优的策略。 &lt;br /&gt;
本文在传统理论的基础上提出了，大脑前额叶皮层在学习中也发挥着重要作用，并和多巴胺相互作用，从而发现存在的一种学习范式元强化学习。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;本文将大脑前额叶皮层抽象为一个RNN模型，通过仿真实验观察前额叶皮层和多巴胺在学习过程的活动，仿真实验基于一组相关的学习任务进行。本文提出的新的理论解释为：基于多巴胺调节的学习机制是一种model-free的学习机制，该机制用来调节突触权重。而在前额叶皮层中存在着另外一种model-based的学习机制，在这一过程中，突触权重是固定的，而通过调整神经网络中的激活状态来适应不同任务。这两个学习过程结合起来，被称为元强化学习过程。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文基于6个仿真实验来显示新的理论，主要任务基于Harlow Task（猴子学习实验，给猴子同时展示左右两个不同的物体，选择其中一个会有食物奖励，另一个没有，两个物体出现的位置随机，多次后，猴子便可以学习到该规则，当更换不同物体时，猴子通过尝试也能快速适应，即学会学习）。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文通过Harlow Task仿真实验，训练一个RNN模型（对大脑前额叶皮层的建模）使其产生了元学习能力，从而解释了基于多巴胺和大脑前额叶皮层的元学习的生物学机制。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 传统的强化学习理论的生物学基础是通过多巴胺传递奖励误差信号，从而动态调整神经突触的连接，来进行学习，达到最优的策略。 本文在传统理论的基础上提出了，大脑前额叶皮层在学习中也发挥着重要作用，并和多巴胺相互作用，从而发现存在的一种学习范式元强化学习。 实现过程 本文将大脑前额叶皮层抽象为一个RNN模型，通过仿真实验观察前额叶皮层和多巴胺在学习过程的活动，仿真实验基于一组相关的学习任务进行。本文提出的新的理论解释为：基于多巴胺调节的学习机制是一种model-free的学习机制，该机制用来调节突触权重。而在前额叶皮层中存在着另外一种model-based的学习机制，在这一过程中，突触权重是固定的，而通过调整神经网络中的激活状态来适应不同任务。这两个学习过程结合起来，被称为元强化学习过程。 测试任务 本文基于6个仿真实验来显示新的理论，主要任务基于Harlow Task（猴子学习实验，给猴子同时展示左右两个不同的物体，选择其中一个会有食物奖励，另一个没有，两个物体出现的位置随机，多次后，猴子便可以学习到该规则，当更换不同物体时，猴子通过尝试也能快速适应，即学会学习）。 创新点 本文通过Harlow Task仿真实验，训练一个RNN模型（对大脑前额叶皮层的建模）使其产生了元学习能力，从而解释了基于多巴胺和大脑前额叶皮层的元学习的生物学机制。 算法评价 返回文章列表</summary></entry><entry><title type="html">Paper reading: Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning</title><link href="http://localhost:4000/2021/06/06/savn.html" rel="alternate" type="text/html" title="Paper reading: Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning" /><published>2021-06-06T00:00:00+08:00</published><updated>2021-06-06T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/06/savn</id><content type="html" xml:base="http://localhost:4000/2021/06/06/savn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;本文是meta-RL在navigation中的应用。  &lt;br /&gt;
本文将navigation任务的学习分为了三个层次：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learn：通过监督式actor-critic loss进行学习&lt;/li&gt;
  &lt;li&gt;learn to learn：通过自适应算法的interactive loss进行学习&lt;/li&gt;
  &lt;li&gt;learn to how to learn:通过学习自适应算法的interactive loss进行学习&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文提出的SAVN(self-adapted visual navigation)在传统的强化学习算法（如A2C）的基础上，添加了基于梯度下降的元学习算法（based on MAML）和自监督学习机制，让agent可以在通过与环境交互来学习的基础上，达成其他两个更高层次的能力：学会学习，学会如何学习。&lt;/p&gt;

&lt;p&gt;通俗的说，让agent学会了在新环境中随机应变的能力，使其可以快速适应新环境。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;在训练过程中，通过最小化interactive loss来学习自适应网络，在测试过程中，固定自适应网络参数，只学习优化navigation loss。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文在AI-thor的3D navigation任务上进行了测试，任务是只基于RGB观测信息导航agent到任一指定物品处。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;在navigation中加入meta-RL的概念，通过自监督学习使agent在inference阶段也可以学习更新策略。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;本文在传统RL navigation的基础上加入了meta learning和自监督学习的概念，是meta-RL在navigation的一个不错的应用。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 本文是meta-RL在navigation中的应用。 本文将navigation任务的学习分为了三个层次： learn：通过监督式actor-critic loss进行学习 learn to learn：通过自适应算法的interactive loss进行学习 learn to how to learn:通过学习自适应算法的interactive loss进行学习 本文提出的SAVN(self-adapted visual navigation)在传统的强化学习算法（如A2C）的基础上，添加了基于梯度下降的元学习算法（based on MAML）和自监督学习机制，让agent可以在通过与环境交互来学习的基础上，达成其他两个更高层次的能力：学会学习，学会如何学习。 通俗的说，让agent学会了在新环境中随机应变的能力，使其可以快速适应新环境。 实现过程 在训练过程中，通过最小化interactive loss来学习自适应网络，在测试过程中，固定自适应网络参数，只学习优化navigation loss。 测试任务 本文在AI-thor的3D navigation任务上进行了测试，任务是只基于RGB观测信息导航agent到任一指定物品处。 创新点 在navigation中加入meta-RL的概念，通过自监督学习使agent在inference阶段也可以学习更新策略。 算法评价 本文在传统RL navigation的基础上加入了meta learning和自监督学习的概念，是meta-RL在navigation的一个不错的应用。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</title><link href="http://localhost:4000/2021/06/05/dream.html" rel="alternate" type="text/html" title="Paper reading: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices" /><published>2021-06-05T00:00:00+08:00</published><updated>2021-06-05T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/05/dream</id><content type="html" xml:base="http://localhost:4000/2021/06/05/dream.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;想象要学习一门新的编程语言，一种做法是找一本教科书或找到官方文档，完整阅读一遍，然后开始编程，但这也许不是最高效的做法。更高效的做法是找到一个要解决的问题或要实现的算法，尝试写出伪代码，然后学习这门语言的语法，并用其实现这个算法，也就是在实践中去学习。这个过程可以抽象成一个exploration和exploitation的过程。  &lt;br /&gt;
在meta-RL中，存在如何解耦exploration和exploitation的问题，如教会一个机器人做饭，我们可以让其在一些厨房场景中先学习如何做饭，然后将其放在新的场景下，让其快速适应并完成任务。在新的场景中，机器人要找到食材和厨具（exploration），然后做出一道可口的菜（exploitation）。  &lt;br /&gt;
虽然通常的meta-RL的算法对exploration和exploitation进行了解耦，但在explore的时候往往会收集到任务无关的信息而陷入局部最优解。  &lt;br /&gt;
DREAM的核心思想是构建独立的exploitation目标来确定任务相关的信息，然后构建独立的explore目标来寻找这些信息，从而实现最优解。通俗的说，就是带着目的去探索，然后用探索到的信息去完成任务。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;在meta-training的trial中，针对于一个场景问题$\mu$，Dream训练一个任务编码器$F_{\psi}$，该编码器可以提取出$\mu$中的任务相关的信息，利用$F_{\psi}$作为中间件将exploration和exploitation进行解耦。  &lt;br /&gt;
定义相关信息函数$I$，在exploration的过程中，通过更新exploration的策略来最大化该相关信息函数，从而得到最优exploration策略。 &lt;br /&gt;
在exploitation的过程中，通过更新exploitation的策略和$F_{\psi}$来最大化exploitation奖励，从而得到最优exploitation策略。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;Dream分别在定制的2D navigation环境和基于gym-miniworld的3D navigation环境上进行了测试。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文提出了一个任务编码器，可以从问题中提取出任务相关信息，并排除任务无关信息，从而通过解耦exploration和exploitation，分别对两个过程进行优化得到最优策略。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;Dream针对于端到端的meta-train方法容易陷入局部最优并且需要大量训练样本的问题，对exploration和exploitation进行了解耦，从而可以分别对这两个过程的策略进行优化。本文对问题定义很清晰，算法也很符合直觉。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 想象要学习一门新的编程语言，一种做法是找一本教科书或找到官方文档，完整阅读一遍，然后开始编程，但这也许不是最高效的做法。更高效的做法是找到一个要解决的问题或要实现的算法，尝试写出伪代码，然后学习这门语言的语法，并用其实现这个算法，也就是在实践中去学习。这个过程可以抽象成一个exploration和exploitation的过程。 在meta-RL中，存在如何解耦exploration和exploitation的问题，如教会一个机器人做饭，我们可以让其在一些厨房场景中先学习如何做饭，然后将其放在新的场景下，让其快速适应并完成任务。在新的场景中，机器人要找到食材和厨具（exploration），然后做出一道可口的菜（exploitation）。 虽然通常的meta-RL的算法对exploration和exploitation进行了解耦，但在explore的时候往往会收集到任务无关的信息而陷入局部最优解。 DREAM的核心思想是构建独立的exploitation目标来确定任务相关的信息，然后构建独立的explore目标来寻找这些信息，从而实现最优解。通俗的说，就是带着目的去探索，然后用探索到的信息去完成任务。 实现过程 在meta-training的trial中，针对于一个场景问题$\mu$，Dream训练一个任务编码器$F_{\psi}$，该编码器可以提取出$\mu$中的任务相关的信息，利用$F_{\psi}$作为中间件将exploration和exploitation进行解耦。 定义相关信息函数$I$，在exploration的过程中，通过更新exploration的策略来最大化该相关信息函数，从而得到最优exploration策略。 在exploitation的过程中，通过更新exploitation的策略和$F_{\psi}$来最大化exploitation奖励，从而得到最优exploitation策略。 测试任务 Dream分别在定制的2D navigation环境和基于gym-miniworld的3D navigation环境上进行了测试。 创新点 本文提出了一个任务编码器，可以从问题中提取出任务相关信息，并排除任务无关信息，从而通过解耦exploration和exploitation，分别对两个过程进行优化得到最优策略。 算法评价 Dream针对于端到端的meta-train方法容易陷入局部最优并且需要大量训练样本的问题，对exploration和exploitation进行了解耦，从而可以分别对这两个过程的策略进行优化。本文对问题定义很清晰，算法也很符合直觉。 返回文章列表</summary></entry><entry><title type="html">Paper reading: On First-Order Meta-Learning Algorithms</title><link href="http://localhost:4000/2021/06/04/Reptile.html" rel="alternate" type="text/html" title="Paper reading: On First-Order Meta-Learning Algorithms" /><published>2021-06-04T00:00:00+08:00</published><updated>2021-06-04T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/04/Reptile</id><content type="html" xml:base="http://localhost:4000/2021/06/04/Reptile.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;Reptile和MAML一样都是模型无关的元学习算法，不同的是，Reptile是用在新任务上的梯度下降方向和初始参数的方向求差作为初始参数的最终更新方向。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/post_assets/2021-06-04/gradient-update-reptile.jpg&quot; alt=&quot;pre-erain、MAML、Reptile的梯度更新方向&quot; /&gt;
  &lt;figcaption&gt;Pre-erain、MAML、Reptile的梯度更新方向的示意图。$\theta$为初始参数，蓝色箭头为不同方法初始参数更新方向，绿色箭头为初始参数在采样的任务上的梯度下降方向。&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;算法流程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;采样一个任务或一批任务。&lt;/li&gt;
  &lt;li&gt;在任务上用初始化参数基于梯度下降训练，得到所有梯度方向，并求出总的梯度下降方向。&lt;/li&gt;
  &lt;li&gt;基于初始参数方向和总梯度下降方向的差作为参数更新方向。
    &lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;作者针对于few-shot classification做了实验，效果和MAML差不多。
    &lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
    &lt;p&gt;Reptile是基于MAML做了稍微改进，用梯度下降方向和初始参数方向的差作为更新方向，优势在于兼顾了所有任务的更新，降低了overfit的风险。&lt;/p&gt;
    &lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
    &lt;p&gt;Reptile基于MAML做了微小改动，核心思想并没有太大差别。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 Reptile和MAML一样都是模型无关的元学习算法，不同的是，Reptile是用在新任务上的梯度下降方向和初始参数的方向求差作为初始参数的最终更新方向。 实现过程 Pre-erain、MAML、Reptile的梯度更新方向的示意图。$\theta$为初始参数，蓝色箭头为不同方法初始参数更新方向，绿色箭头为初始参数在采样的任务上的梯度下降方向。 算法流程： 采样一个任务或一批任务。 在任务上用初始化参数基于梯度下降训练，得到所有梯度方向，并求出总的梯度下降方向。 基于初始参数方向和总梯度下降方向的差作为参数更新方向。 测试任务 作者针对于few-shot classification做了实验，效果和MAML差不多。 创新点 Reptile是基于MAML做了稍微改进，用梯度下降方向和初始参数方向的差作为更新方向，优势在于兼顾了所有任务的更新，降低了overfit的风险。 算法评价 Reptile基于MAML做了微小改动，核心思想并没有太大差别。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title><link href="http://localhost:4000/2021/06/03/MAML.html" rel="alternate" type="text/html" title="Paper reading: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" /><published>2021-06-03T00:00:00+08:00</published><updated>2021-06-03T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/03/MAML</id><content type="html" xml:base="http://localhost:4000/2021/06/03/MAML.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 &lt;br /&gt;
MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。  &lt;br /&gt;
由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
&lt;p&gt;MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 MAML是一种基于梯度下降的元学习算法，可以被用在分类和回归（监督学习）及强化学习中，来加速对新任务的学习。作者将不同的学习任务抽象为few-shot learning，也就是用少量学习样本就可以快速适应新任务。不同类型的任务除了损失函数的定义、数据的获取和表征外，基本的训练方法都是一致的，因此这种算法可以适用于不同任务。 MAML的核心思想是针对一系列学习任务构建共有的中间表征，不具体构造出新的模块来进行few-shot learning，而是对于已有的模型（如神经网络）的初始参数，找到模型种对不同任务都很敏感的参数，然后通过一个或多个梯度下降步长优化这些参数，从而在新任务上快速降低损失函数，获得最优性能。 实现过程 以K-shot-meta-RL（K是每个任务的样本数量）任务的MAML算法为例，对于一个任意任务分布，从中采样出一批任务用来进行元学习。对于每一个任务，用初始策略$f_{\theta}$获得K个样本轨迹$\mathcal{D}$，计算基于$f_{\theta}$和$\mathcal{D}$的损失函数，损失函数用来衡量在$\mathcal{D}$的最大奖励的期望。然后更新梯度，并用新的梯度重新获取K个样本轨迹用于下一步优化。 由于强化学习的损失函数不可微分，所以使用策略梯度算法进行求解和优化。 测试任务 在RL任务种，测试任务为rllab（已弃用，现在是garage）的2D navigation和locomotion，其中locomotion的物理引擎是mujoco。 创新点 MAML从不同的学习范式种抽象出共同特征，从而可以用统一的算法进行few-shot learning。另外，基于梯度下降的算法思想很简单。 算法评价 MAML是比较接近meta learning本质的算法，也就是寻找任务的共同中间表征从而达到泛化的目的。但是MAML缺点是计算量很大，结合启发式的方法或许可以降低计算量，比如将梯度下降算法和启发式的因果模型结合起来，或许只需要1个梯度下降就可以找到最优解。 返回文章列表</summary></entry><entry><title type="html">Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning</title><link href="http://localhost:4000/2021/06/02/RL2.html" rel="alternate" type="text/html" title="Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning" /><published>2021-06-02T00:00:00+08:00</published><updated>2021-06-02T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/02/RL2</id><content type="html" xml:base="http://localhost:4000/2021/06/02/RL2.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;episode:&lt;/strong&gt; agent和一个MDP的一次开始到结束的交互序列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;trial:&lt;/strong&gt; agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。&lt;/p&gt;

&lt;p&gt;agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了&lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;TRPO算法&lt;/a&gt;。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-armed Bandit&lt;/li&gt;
  &lt;li&gt;Tabular MDP&lt;/li&gt;
  &lt;li&gt;3D maze navigation(ViZDoom environment)
    &lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
    &lt;p&gt;这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。&lt;/p&gt;
    &lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;
    &lt;p&gt;这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。 实现过程 定义 episode: agent和一个MDP的一次开始到结束的交互序列。 trial: agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。 agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了TRPO算法。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。 测试任务 Multi-armed Bandit Tabular MDP 3D maze navigation(ViZDoom environment) 创新点 这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。 算法评价 这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。 返回文章列表</summary></entry><entry><title type="html">Paper reading: Learning to reinforcement learn</title><link href="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html" rel="alternate" type="text/html" title="Paper reading: Learning to reinforcement learn" /><published>2021-06-01T00:00:00+08:00</published><updated>2021-06-01T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn</id><content type="html" xml:base="http://localhost:4000/2021/06/01/Learning-to-reinforcement-learn.html">&lt;h2 id=&quot;核心思想&quot;&gt;核心思想&lt;/h2&gt;
&lt;p&gt;本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。&lt;/p&gt;
&lt;h2 id=&quot;实现过程&quot;&gt;实现过程&lt;/h2&gt;
&lt;p&gt;本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。&lt;/p&gt;

&lt;p&gt;训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。&lt;/p&gt;
&lt;h2 id=&quot;测试任务&quot;&gt;测试任务&lt;/h2&gt;
&lt;p&gt;本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的&lt;a href=&quot;https://scienceofbehaviorchange.org/measures/two-stage-task/&quot;&gt;two step task&lt;/a&gt;和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。&lt;/p&gt;
&lt;h2 id=&quot;创新点&quot;&gt;创新点&lt;/h2&gt;
&lt;p&gt;本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。&lt;/p&gt;
&lt;h2 id=&quot;算法评价&quot;&gt;算法评价&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://localhost:4000/2021/05/29/meta-reinforcement-learning.html&quot;&gt;返回文章列表&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="paper" /><category term="review" /><summary type="html">核心思想 本文提出了一种深度元强化学习(deep meta-reinforcement learning)方法，通过强化学习训练一个循环神经网络(RNN)，使得该RNN可以通过自适应更新的方法快速适应不同任务，从而提高训练效率。 实现过程 本文使用A2C(Advantage Actor-Critic)算法在一系列输入任务上训练一个RNN。以MDPs(Markov Decision Processes)为例，设$\mathcal{D}$是一个MDPs分布，每个新的episode开始时，先从D采样的一个任务$m \backsim \mathcal{D}$，RNN的内部循环单元的参数会被重置，然后和环境交互一定的时长，每一次策略都是基于到目前的所有历史信息（状态，动作，奖励），agent的目标是在所有任务上最大化总的奖励。 训练结束后，网络权重会被固定，然后在一组来自$\mathcal{D}$（或者对分布$\mathcal{D}$稍微修改）的MDPs上测试agent的适应能力。 测试任务 本文分布在4个bandit problem和2个MDP任务上进行了测试，MDP任务分别是一个改编的two step task和一个改编的Harlow task，最后讨论了在一个3D迷宫任务(来自deepmind lab)上进行深度元学习的方法。 创新点 本文是基于模型的元强化学习算法的一次尝试，通过将带LSTM的RNN用强化学习算法进行训练，可以使agent获得一定的学习适应能力。 算法评价 返回文章列表</summary></entry><entry><title type="html">Summary: Reinforcement learning benchmarks</title><link href="http://localhost:4000/2021/05/30/RL-benchmark.html" rel="alternate" type="text/html" title="Summary: Reinforcement learning benchmarks" /><published>2021-05-30T00:00:00+08:00</published><updated>2021-05-30T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/30/RL-benchmark</id><content type="html" xml:base="http://localhost:4000/2021/05/30/RL-benchmark.html">&lt;p&gt;这是一篇强化学习benchmark的总结，按照任务对其进行分类，并不全面地列举了在benchmark测试过的文章及开源代码。&lt;/p&gt;
&lt;h2 id=&quot;2d-navigation&quot;&gt;2D navigation&lt;/h2&gt;
&lt;h3 id=&quot;gym-minigrid&quot;&gt;gym-minigrid&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;source code: &lt;a href=&quot;https://github.com/maximecb/gym-minigrid&quot;&gt;https://github.com/maximecb/gym-minigrid&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;scene screenshot:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/post_assets/2021-05-30/minigrid-shot.gif&quot; alt=&quot;gym-minigrid-shot&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use cases:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.07882&quot;&gt;Guiding Policies with Language via Meta-Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gym-maze&quot;&gt;gym-maze&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;source code: &lt;a href=&quot;https://github.com/MattChanTK/gym-maze&quot;&gt;https://github.com/MattChanTK/gym-maze&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;scene screenshot:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/post_assets/2021-05-30/gymmaze-shot.gif&quot; alt=&quot;gym-maze-shot&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use cases:
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!---   
### 
- source code: 
- scene screenshot: 
- use cases:
  - 
-----&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/clvrai/awesome-rl-envs#navigation&quot;&gt;https://github.com/clvrai/awesome-rl-envs#navigation&lt;/a&gt;&lt;/p&gt;</content><author><name>tianyun ma</name><email>himarsmty@gmail.com</email></author><category term="RL-benchmark" /><summary type="html">这是一篇强化学习benchmark的总结，按照任务对其进行分类，并不全面地列举了在benchmark测试过的文章及开源代码。 2D navigation gym-minigrid source code: https://github.com/maximecb/gym-minigrid scene screenshot: use cases: Guiding Policies with Language via Meta-Learning gym-maze source code: https://github.com/MattChanTK/gym-maze scene screenshot: use cases: Reference: https://github.com/clvrai/awesome-rl-envs#navigation</summary></entry></feed>