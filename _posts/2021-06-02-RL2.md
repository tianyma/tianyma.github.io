---
layout: article
title: "Paper reading: $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
date: 2021-06-02
tags: paper review
mathjax: true
key: 2021-06-02-blog
weight: 0
---
## 核心思想
用经典RL算法训练一个带有GRU结构的RNN-based的agent，对每个固定MDP训练多个episode，在较早的episode可以学习到meta信息，从而在后面的episode中更快适应任务。
## 实现过程
### 定义   

**episode:** agent和一个MDP的一次开始到结束的交互序列。

**trial:** agent与某一固定MDP的一系列连续的episode过程，在本文中，一个trial中包含n个episode。trial中的episode之间，agent的中间状态会被保留。


agent是一个带有GRU结构的RNN，在每个trial中与环境进行交互，agent的目标是最大化每个trial的奖励，策略更新采用了[TRPO算法](https://arxiv.org/pdf/1502.05477.pdf)。在较早的episode训练后，由于agent的中间状态会被保留，所以在之后的episode中，agent会更快速地适应任务。
## 测试任务
- Multi-armed Bandit
- Tabular MDP
- 3D maze navigation(ViZDoom environment)
## 创新点
这篇文章的第三个实验，也就是3D maze navigation很有意思，通过较早的exploration，agent似乎可以在后面的episode中exploit并可以找到最优路径。
## 算法评价
这篇论文似乎没有什么创新点，使用了一个RNN-based的agent，对每一个MDP进行多个episode训练，从而在后面的episode中可以快速适应任务。但是对算法描述很简单，并没有说清楚要学习的meta是什么。

[返回文章列表]({{ site.url}}/2021/05/29/meta-reinforcement-learning.html)