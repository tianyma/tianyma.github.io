---
layout: article
title: "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices"
date: 2021-06-05
tags: paper review
mathjax: true
key: 2021-06-05-blog
---
## 核心思想
想象要学习一门新的编程语言，一种做法是找一本教科书或找到官方文档，完整阅读一遍，然后开始编程，但这也许不是最高效的做法。更高效的做法是找到一个要解决的问题或要实现的算法，尝试写出伪代码，然后学习这门语言的语法，并用其实现这个算法，也就是在实践中去学习。这个过程可以抽象成一个exploration和exploitation的过程。    
在meta-RL中，存在如何解耦exploration和exploitation的问题，如教会一个机器人做饭，我们可以让其在一些厨房场景中先学习如何做饭，然后将其放在新的场景下，让其快速适应并完成任务。在新的场景中，机器人要找到食材和厨具（exploration），然后做出一道可口的菜（exploitation）。    
虽然通常的meta-RL的算法对exploration和exploitation进行了解耦，但在explore的时候往往会收集到任务无关的信息而陷入局部最优解。    
DREAM的核心思想是构建独立的exploitation目标来确定任务相关的信息，然后构建独立的explore目标来寻找这些信息，从而实现最优解。通俗的说，就是带着目的去探索，然后用探索到的信息去完成任务。
## 实现过程
在meta-training的trial中，针对于一个场景问题$\mu$，Dream训练一个任务编码器$F_{\psi}$，该编码器可以提取出$\mu$中的任务相关的信息，利用$F_{\psi}$作为中间件将exploration和exploitation进行解耦。    
定义相关信息函数$I$，在exploration的过程中，通过更新exploration的策略来最大化该相关信息函数，从而得到最优exploration策略。   
在exploitation的过程中，通过更新exploitation的策略和$F_{\psi}$来最大化exploitation奖励，从而得到最优exploitation策略。
## 测试任务
Dream分别在定制的2D navigation环境和基于gym-miniworld的3D navigation环境上进行了测试。
## 创新点
本文提出了一个任务编码器，可以从问题中提取出任务相关信息，并排除任务无关信息，从而通过解耦exploration和exploitation，分别对两个过程进行优化得到最优策略。
## 算法评价
Dream针对于端到端的meta-train方法容易陷入局部最优并且需要大量训练样本的问题，对exploration和exploitation进行了解耦，从而可以分别对这两个过程的策略进行优化。本文对问题定义很清晰，算法也很符合直觉。

[返回文章列表](https://tianyma.github.io/2021/05/29/meta-reinforcement-learning.html)